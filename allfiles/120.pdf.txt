IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

1061

Sequential Detection of Market Shocks
With Risk-Averse CVaR Social Sensors

Vikram Krishnamurthy, Fellow, IEEE, and Sujay Bhatt

AbstractThis paper considers a statistical signal processing
problem involving agent-based models of nancial markets, which
at a microlevel are driven by socially aware and risk-averse agents.
These agents trade (buy or sell) stocks at each trading instant by
using the decisions of all previous agents (social learning) in addi-
tion to a private (noisy) signal they receive on the value of the
stock. We are interested in the following: 1) modelling the dynam-
ics of these risk averse agents and 2) sequential detection of a
market shock based on the behaviour of these agents. Structural
results that characterize social learning under a risk measure,
conditional value-at-risk (CVaR), are presented and formulation
of the Bayesian change point detection problem is provided. The
structural results exhibit two interesting properties: 1) risk averse
agents herd more often than risk neutral agents and 2) the stop-
ping set in the sequential detection problem is nonconvex. The
framework is validated on data from the Yahoo! Tech Buzz game
dataset and it is revealed that 1) the model identies the value
changes based on agents trading decisions. 2) Reasonable quickest
detection performance is achieved when the agents are risk-averse.

Index TermsConditional value at risk (CVaR), social learn-
ing lter, market shock, quickest detection, agent based models,
monotone Bayesian update, coherent risk measure, POMDP.

I. INTRODUCTION

F INANCIAL markets evolve based on the behaviour of a

large number of interacting entities. Understanding the
interaction of these agents is therefore essential in statistical
inference from nancial data. This motivates the study of agent
based models for nancial markets. Agent based models are
useful for capturing the global behaviour of highly intercon-
nected nancial systems by simulating the behaviour of the
local interacting systems [1][4]. Unlike standard economic
models which emphasize the equilibrium properties of nan-
cial markets, agent based models stress local interactions and
out-of-equilibrium dynamics that may not reach equilibrium in
the long run [5]. Agent based models are commonly used to
determine the conditions that lead a group of interacting agents
to form an aggregate behaviour [6][9] and to model stylized
facts like correlation of returns and volatility clustering [10],
[11]. Agent based models have also been used model anoma-
lies that the standard approaches fail to explain like fat tails,
absence of simple arbitrage, gain/loss asymmetry and leverage
effects [12], [13].

Manuscript received October 14, 2015; revised February 16, 2016; accepted
March 16, 2016. Date of publication April 14, 2016; date of current version
August 12, 2016. The guest editor coordinating the review of this manuscript
and approving it for publication was Dr. Emmanuelle Jay.

The authors are with the Department of Electrical and Computer
Engineering, University of British Columbia, Vancouver, BC V6T 1Z4, Canada
(e-mail: vikramk@ece.ubc.ca; sujaybhatt@ece.ubc.ca).

Digital Object Identier 10.1109/JSTSP.2016.2548995

In this paper, we are interested in developing agent based
models for studying global events in nancial markets where
the underlying value of the stock experiences a jump change
(shock). Market shocks are known to affect stock market returns
[14], cause uctuations in the economy [15] and necessitate
market making [16]. Therefore detecting shocks is essential and
when the interacting agents are acting based on private signals
and complete history of other agents trading decisions, it is
non-trivial [17].

The problem of market shock detection in the presence of
social learning considered in this paper is different from a
standard signal processing (SP) problem in the following ways:
1) Agents (or social sensors) inuence the behaviour of other
agents, whereas in standard SP sensors typically do not
affect other sensors.

2) Agents reveal quantized information (decisions) and have
dynamics, whereas in standard SP sensors are static with
the dynamics modelled in the state equation.

3) Standard SP is expectation centric. In this paper we use
coherent risk measures which generalizes the concept of
expected value and is much more relevant in nancial
applications. Such coherent risk measures [18] are now
widely used in nance to model risk averse behaviour.

Properties 1 and 2 above are captured by social learning mod-
els. Such social learning models, where agents face xed prices,
are considered in [9], [19][21]. They show that after a nite
amount of time, an informational cascade takes place and all
subsequent agents choose the same action regardless of their
private signal. Models where agents act sequentially to opti-
mize local costs (to choose an action) and are socially aware
were considered in [7], [22]. This paper considers a similar
model, but, in order to incorporate property 3 above (risk averse
behaviour), we will replace the classical social learning model
of expected cost minimizers with that of risk averse minimiz-
ers. The resulting risk-averse social learning lter has several
interesting (and unusual) properties that will be discussed in
the paper.

A. Main Results and Organization

Section II presents the social learning agent based model and
the market observers objective for detecting shocks. The for-
mulation involves the interaction of local and global decision
makers. Individual agents perform social learning and the mar-
ket observer seeks to determine if the underlying asset value has
changed based on the agent behaviour. The shock in the asset
value changes at a phase distributed time (which generalizes

1932-4553  2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.

See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

1062

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

geometric distributed change times). The problem of market
shock detection considered in this paper is different from the
classical Bayesian quickest detection [23][25] where, local
observations are used to detect the change. Quickest detection
in the presence of social learning was considered in [17] where
it was shown that making global decisions (stop or continue)
based on local decisions (buy or sell) leads to discontinuous
value function and the optimal policy has multiple thresholds.
However, unlike [17] which deals with expected cost, we con-
sider a more general measure to account for the local agents
attitude towards risk.

It is well documented in various elds like economics [26],
behavioural economics, psychology [27] that people prefer a
certain but possibly less desirable outcome over an uncer-
tain but potentially larger outcome. To model this risk averse
behaviour, commonly used risk measures1 are Value-at-Risk
(VaR), Conditional Value-at-Risk (CVaR), Entropic risk mea-
sure and Tail value at risk; see [28]. We consider social learning
under CVaR risk measure. CVaR [29] is an extension of VaR
that gives the total loss given a loss event and is a coherent risk
measure [18]. In this paper, we choose CVaR risk measure as
it exhibits the following properties [18], [29]: (i) It associates
higher risk with higher cost. (ii) It ensures that risk is not a func-
tion of the quantity purchased, but arises from the stock. (iii) It
is convex. CVaR as a risk measure has been used in solving
portfolio optimization problems [30], [31] credit risk optimiza-
tion [32] and also order execution [33]. For an overview of risk
measures and their application in nance, see [28].

Section III provides structural results which characterize the
social learning under CVaR risk measure and its properties. We
show that, under reasonable assumptions on the costs, the trad-
ing decisions taken by socially aware and risk-averse agents are
ordinal functions of their private observations and monotone
in the prior information. This implies that the Bayesian social
learning follows simple intuitive rules. The change point detec-
tion problem is formulated as a Market Observer2 seeking to
detect a shock in the stock value (modelled as a Markov chain)
by balancing the natural trade-off between detection delay and
false alarm.

Section IV discusses the unusual properties exhibited by
the CVaR social learning lter and explores the link between
local and global behaviour in agent based models for detec-
tion of market shocks. We show that the stopping region for
the sequential detection problem is non-convex; this is in con-
trast to standard signal processing quickest detection problems
where the stopping set is convex.

Finally, Section V discusses an application of the agent based
model and change detection framework in a stock market data
set. We use a data set from Tech Buzz Game which is a stock
1A risk measure  : L  R is a mapping from the space of measurable func-
tions to the real line which satises the following properties: (i) (0) = 0. (ii) If
S1, S2  L and S1  S2 a.s then (S1)  (S2). (iii) if a  R and S  L,
then (S + a) = (S) + a. The risk measure is coherent if in addition  satis-
es: (iv) If S1, S2  L, then (S1 + S2)  (S1) + (S1). (v) If a  0 and
S  L, then (aS) = a(S). The expectation operator is a special case where
subadditivity is replaced by additivity.

2Market observer could be, for example, a securities dealer (investment bank
or syndicate) that underwrites the stock which is later traded in a secondary
market.

market simulation launched by Yahoo! Research and OReilly
Media on March 15, 2005 to gain insights into forecasting high-
tech events and trades. The game uses Dynamic pari-mutuel
markets (DPM) as its trading mechanism. DPMs are known to
provide accurate predictions in eld studies on  price forma-
tion in election stock markets [34], mechanism design for sales
forecasting [35] and betting in sports markets [36], [37].

II. CVAR SOCIAL LEARNING MODEL AND MARKET

OBSERVERS OBJECTIVE

This section presents the Bayesian social learning model and
denes the objective of the market observer. As will be shown
later in Section III, the model results in ordinal decision mak-
ing thereby mimicking human behavior and the risk measure
captures a traders attitude towards risk.

A. CVaR Social Learning Model

The market micro-structure is modelled as a discrete time
dealer market motivated by algorithmic and high-frequency
tick-by-tick trading [38]. There is a single traded stock or
asset, a market observer and a countable number of trad-
ing agents. The asset has an initial true underlying value
x0  X = {1, 2, . . . , X}. The market observer does not receive
direct information about x  X but only observes the pub-
lic buy/sell actions of agents, ak  A = {1(buy), 2(sell)}. The
agents themselves receive noisy private observations of the
underlying value x and consider this in addition to the trad-
ing decisions of the other agents visible in the order book [39],
[40], [41]. At a random time,  0 determined by the transition
matrix P , the asset experiences a jump change in its value to
a new value. The aim of the market observer is to detect the
change time (global decision) with minimal cost, having access
to only the actions of these socially aware agents. Let yk  Y =
{1, 2, . . . , Y } denote agent ks private observation. The initial
distribution is 0 = (0(i), i  X ) where 0(i) = P(x0 = i).

The agent based model has the following dynamics:
1. Shock in the asset value: At

time  0 > 0,

the asset
experiences a jump change (shock) in its value due to
exogenous factors. The change point  0 is modelled by
a phase type (PH) distribution. The family of all PH-
distributions forms a dense subset for the set of all
distributions [42] i.e., for any given distribution func-
tion F such that F (0) = 0, one can nd a sequence
of PH-distributions {Fn, n  1} to approximate F uni-
formly over [0,). The PH-distributed time  0 can be
constructed via a multi-state Markov chain xk with state
space X = {1, . . . , X} as follows: Assume state 1 is
an absorbing state and denotes the state after the jump
change. The states 2, . . . , X (corresponding to beliefs
e2, . . . , eX) can be viewed as a single composite state that
x resides in before the jump. So  0 = inf {k : xk = 1}
and the transition probability matrix P is of the form

(cid:3)

(1)

(cid:2)

P =

1

0

P (X1)1 P(X1)(X1)

KRISHNAMURTHY AND BHATT: SEQUENTIAL DETECTION OF MARKET SHOCKS

1063

The distribution of the absorption time to state 1 is
k  1,

0 = 0(1),

k = 

(2)

0 P k1P ,
(cid:2)
(cid:2)

where 0 = [0(2), . . . , 0(X)]
. The key idea is that by
appropriately choosing the pair (0, P ) and the asso-
ciated state space dimension X, one can approximate
any given discrete distribution on [0,) by the distri-
bution {k, k  0}; see [42, pp. 240243]. The event
{xk = 1} means the change point has occurred before
time k according to PH-distribution (2). In the special
case when x is a 2-state Markov chain, the change time
 0 is geometrically distributed.

2. Agents Private Observation: Agent

private
(local) observation denoted by yk is a noisy mea-
surement of the true value of the asset. It is obtained from
the observation likelihood distribution as,
Bxy = P(yk = y|xk = x)

ks

(3)

3. Private Belief update: Agent k updates its private
belief using the observation yk and the prior public
belief k1(i) = P(X = i|a1, . . . , ak1) as the follow-
ing Hidden Markov Model update

k =

(cid:2)

k1
Byk P
1(cid:2)Byk P (cid:2)k1

(4)

where 1 denotes the X-dimensional vector of ones.
4. Agents trading decision: Agent k executes an action
ak  A = {1(buy), 2(sell)} to myopically minimize its
cost. Let c(i, a) denote the cost incurred if the agent takes
action a when the underlying state is i. Let the local cost
vector be

ca = [c(1, a)c(2, a) . . . c(X, a)]

The costs for different actions are taken as

c(i, j) = pj  ij for i  X , j  A

(5)

(6)

where ij corresponds to the agents demand. Here
demand is the agents desire and willingness to trade at
a price pj for the stock. Here p1 is the quoted price for
purchase and p2 is the price demanded in exchange for
the stock. We assume that the price is the same during
the period in which the value changes. As a result, the
willingness of each agent only depends on the degree of
uncertainty on the value of the stock.

Remark 1: The analysis provided in this paper straight-
forwardly extends to the case when different agents are
facing different prices like in an order book [39][41].
For notational simplicity we assume the cost are time
invariant.
The agent considers measures of risk in the presence
of uncertainty in order to overcome the losses incurred
in trading. To illustrate this, let c(x, a) denote the loss
incurred with action a while at unknown and random state
x  X . When an agent solves an optimization problem
involving c(x, a) for selecting the best trading decision,

1


{z +

= argmin
aA

ak = argmin
aA

it will take into account not just the expected loss, but
also the riskiness associated with the trading decision
a. The agent therefore chooses an action ak to minimize
the CVaR measure3 of trading as
{CVaR(c(xk, a))}
{min
zR

(7)
Eyk [max{(c(xk, a)  z), 0}]}}
Here   (0, 1] reects the degree of risk-aversion for the
agent (the smaller  is, the more risk-averse the agent is).
Dene
Hk := - algebra generated by(a1, a2, . . . , ak1, yk)
(8)
Eyk denotes the expectation with respect to private belief,
i.e, Eyk = E[.|Hk] when the private belief is updated after
observation yk.

5. Social Learning and Public belief update: Agent ks
action is recorded in the order book and hence broad-
cast publicly. Subsequent agents and the market observer
update the public belief on the value of the stock accord-
ing to the social learning Bayesian lter as follows

(cid:2)

k1
k1
ak P
ak P (cid:2)k1
k1

R
1(cid:2)R

k1

k = T k1(k1, ak) =
(9)
ak = diag(P(ak|x = i, k1), i  X ), where
P(ak|y, k1)P(y|xk = i) and
aA CVaR(c(xk, a));

1 if ak = argmin

Here, R
P(ak|x = i, k1) =
(cid:5)

P(ak|y, k1) =

(cid:4)

yY

0 otherwise.

Note that k belongs to the unit simplex (X)
X : 1(cid:2)
R

X  = 1, 0    1 for all i  X}.



={ 

6. Market Observers Action: The market observer (securi-
ties dealer) seeks to achieve quickest detection by balanc-
ing delay with false alarm. At each time k, the market
observer chooses action4 uk as

uk  U = {1(stop), 2(continue)}

(10)

Here Stop indicates that the value has changed and the
dealer incorporates this information before selling new
issues to investors. The formulation presented considers
a general parametrization of the costs associated with
detection delay and false alarm costs. Dene
Gk := - algebra generated by (a1, a2, . . . , ak1, ak).
(11)
3For the reader unfamiliar with risk measures, it should be noted that CVaR
is one of the big developments in risk modelling in nance in the last 15
years. In comparison, the value at risk (VaR) is the percentile loss namely,
VaR(x) = min{z : Fx(z)  } for cdf Fx. While CVaR is a coherent risk
measure, VaR is not convex and so not coherent. CVaR has other remarkable
properties [29]: it is continuous in  and jointly convex in (x, ). For continu-
ous cdf Fx, CVaR(x) = E{X|X > VaR(x)}. Note that the variance is not
a coherent risk measure.

4It is important to distinguish between the local decisions ak of the agents
and global decisions uk of the market observer. Clearly the decisions ak
affect the choice of uk as will be made precise below.

1064

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

where  denotes a stationary policy. For each initial distribution
0  (X) and policy , the following cost is associated

1(cid:6)
k1C(k, uk = 2)+1C(k, uk=1)

J(0)=E


0

(cid:5)

(cid:7)

k=1

(15)
Here   [0, 1] is the discount factor which is a measure of
the degree of impatience of the market observer. (As long as f
is non-zero, stopping is guaranteed in nite time and so  = 1
is allowed.)

Given the cost, the market observers objective is to deter-
mine  0 with minimum cost by computing an optimal policy



such that

J(0) = inf


J(0)

(16)

The sequential detection problem (16) can be viewed as a par-
tially observed Markov decision process (POMDP) where the
belief update is given by the social learning lter.

Fig. 1. Sequential detection with risk-averse social sensors. Each social sensor
receives a noisy observation on the state and chooses an action to minimize
its CVaR measure of trading. The social sensors communicate their actions to
subsequent sensors. The market observer seeks to determine if there is a change
in the value of the underlying asset from the actions of the sensors.

The optimal policy of the market observer 

C. Stochastic Dynamic Programming Formulation
: (X) 

{1, 2} is the solution of (15) and is given by Bellmans dynamic
(cid:7)
programming equation as follows:

(cid:5)

C(, 1), C(, 2) + 

V (T (, a))(, a)

V () = min





()=argmin

(cid:5)

(cid:6)

aA

(cid:6)

(cid:7)

(17)

i) Cost of Stopping: The asset experiences a jump
change(shock) in its value at time  0. If the action
uk = 1 is chosen before the change point, a false
alarm penalty is incurred. This corresponds to the
xk = i  uk = 1. Let I denote the indica-
event 
i2
tor function. The cost of false alarm in state i, i  X
with fi  0 is thus given by fiI(xk = i, uk = 1).
The expected false alarm penalty is

C(k, uk = 1) =

fiE{I(xk = i, uk = 1)|Gk}

(cid:6)

iX
= f(cid:2)
k

(12)

C(, 1), C(, 2)+

aA

V (T (, a))(, a)

where f = (f1, . . . , fX ) and it
is chosen with
increasing elements, so that states further from 1
incur higher false alarm penalties. Clearly, f1 = 0.
ii) Cost of delay: A delay cost is incurred when the
event {xk = 1, uk = 2} occurs, i.e, even though the
state changed at k, the market observer fails to
identify the change. The expected delay cost is
C(k, uk = 2) = d E{I(xk = i, uk = 1)|Gk}

(cid:2)
1k

= de

(13)

a P (cid:3)
R
where T (, a) =
a P (cid:3) is the CVaR-social learning l-
1(cid:3)R
(cid:2)
ter and (, a) = 1(cid:2)
R
 is the normalization factor of the
a P
Bayesian update. C(, 1) and C(, 2) from (12) and (13) are
the market observers costs. As C(, 1) and C(, 2) are non-
negative and bounded for   (X), the stopping time  is
nite for all   [0, 1].
(cid:7)
stopping set S = {  (X) : 
S =

() = 1} given by:
(cid:6)

The aim of the market observer is then to determine the

V (T (, a))(, a)

 : C(, 1) < C(, 2) + 

(cid:5)



aA

where d > 0 is the delay cost and e1 denotes the unit
vector with 1 in the rst position.

Fig. 1 illustrates the above social learning model in which the
information exchange between the risk-averse social sensors is
sequential.

B. Market Observers Quickest Detection Objective

The market observer chooses its action at each time k as

The dynamic programming equation (17) is similar to that for
stopping time POMDP except that the belief update is given by
a CVaR social learning lter. As will be shown below, because
of the social learning dynamics, quite remarkably, S is not
necessarily a convex set. This is in stark contrast to classical
quickest detection where the stopping region is always convex
irrespective of the change time distribution [43].

III. PROPERTIES OF CVAR SOCIAL LEARNING FILTER

uk = (k)  {1(stop), 2(continue)}

This section discusses the main results regarding the struc-
tural properties of the CVaR social learning lter and highlights

(14)

KRISHNAMURTHY AND BHATT: SEQUENTIAL DETECTION OF MARKET SHOCKS

1065

the signicant role it plays in charactering the properties of mar-
ket observers value function and optimal policy. According to
Theorem 1, risk-averse agents take decisions that are monotone
and ordinal in the observations and monotone in the prior; and
its monotone ordinal behaviour implies that a Bayesian model
chosen in this paper is a useful idealization.

A. Assumptions

The following assumptions will be used throughout

the

paper:
(A1) Observation matrix B and transition matrix P are TP2 (all

(A2) Agents local cost vector ca is sub-modular. That is

second order minors are non-negative)
c(x, 2)  c(x, 1)  c(x + 1, 2)  c(x + 1, 1).

The matrices being TP2 [44] ensures that the public belief
Bayesian updates can be compared [45] and sub-modular [46]
costs ensure that if it is less risky to choose a = 2 when in x, it
is also less risky to choose it when in x + 1.

B. Properties of CVaR Social Learning Filter

The Y  A local decision likelihood probability matrix R

(analogous to observation likelihood) can be computed as

R = BM , whereM 
y,a



=P(a|y, )

(18)

P(a|y, ) = I(CVaR(c(xk, a)) < CVaR(c(xk, a

)))
= A  {a}. Here I denotes the indicator function.

(cid:2)

(cid:2)

where a

Let H (y, a) = CVaR(c(xk, a)) denote the cost with
CVaR measure, associated with action a and observation y for
convenience i.e,

1


{z +

Ey[max{(c(x, a)  z), 0}]}

H (y, a) = min
(19)
zR
Here Ey = E[.|Hk]. y indicates the dependence of E and
hence H  on the observation. Let a
(, y) = argmin H (y, a)
denote the optimal action of the agent with explicit dependence
on the distribution and observation.



The following result says that agents choose a trading deci-
sion that is monotone and ordinal in their private observation.
Humans typically convert numerical attributes to ordinal scales
before making decisions. For example, it does not matter if the
cost of a meal at a restaurant is $200 or $205; an individual
would classify this cost as high. Also credit rating agencies
use ordinal symbols such as AAA, AA, A.

Theorem 1: Under (A1) and (A2), the action a

(, y) made
by each agent is increasing and hence ordinal in y for any
(, y) is increasing in  with
prior belief . Under (A2), a
respect to the monotone likelihood ratio order (Denition 1 in
the appendix).



The proof is given in the appendix. Theorem 1 says that
agents exhibit monotone ordinal behaviour. The condition that

(, y) is monotone in the observation y is required to char-
a
acterize the local decision matrices on different regions in the
belief space which is stated next.

Theorem 2: Under (A1) and (A2), there are at most Y + 1
distinct local decision likelihood matrices R and the belief



space (X) can be partitioned into the following Y + 1
polytopes:

1 = {  (X) : H(1, 1)  H(1, 2)  0}
P 

P 
l = {  (X) : H(l  1, 1)  H(l  1, 2) < 0 (20)

 H(l, 1)  H(l, 2)  0}, l = 2, . . . , Y
P 
Y +1 = {  (X) : H(Y, 1)  H(Y, 2) < 0}

Also, the matrices R are constant (with respect to ) on each
of these polytopes.

The proof is given in the appendix. Theorem 2 is required
to specify the policy for the market observer. Indeed it leads
to unusual behavior (non-convex) stopping regions in quickest
detection as described in Section IV-B.

IV. SOCIAL LEARNING AND CHANGE DETECTION FOR

RISK-AVERSE AGENTS

This section illustrates the properties of the risk-averse social
learning lter which leads to a non-convex value function and
therefore non-convex stopping set of quickest detection.

A. Social Learning Behavior of Risk Averse Agents
The following discussion highlights the relation between
risk-aversion factor  and the regions P 
l . For a given risk-
aversion factor , Theorem 2 shows that there are at most Y + 1
polytopes on the belief space. It was shown in [17] that for the
risk neutral case with X = 2, and P = I (the value is a random
1 and P 
variable) the intervals P 
3 correspond to the herding
region and the interval P 
2 corresponds to the social learning
region. In the herding region, the agents take the same action as
the belief is frozen. In the social learning region there is obser-
vational learning. However, when the agents are optimizing a
more general risk measure (CVaR), the social learning region is
different for different risk-aversion factors. The social learning
region for the CVaR risk measure is shown in Fig. 2. It can be
observed from Fig. 2 that P 
2 becomes
smaller and P 
3 becomes larger as  decreases. The following
(cid:2)
parameters were chosen:

1 becomes smaller, P 
(cid:3)
(cid:2)

(cid:3)

(cid:3)

(cid:2)

B =

0.8 0.2
0.3 0.7

, P =

1 0
0 1

, c =

1 2
3 0.5

This can be interpreted as risk-averse agents showing a larger
tendency to go with the crowd rather than risk choosing
the other action. With the same B and c parameters, but with
transition matrix

(cid:2)

(cid:3)

P =

1
0
0.1 0.9

the social learning region is shown in Fig. 3. From Fig. 3, it is
observed that when the state is evolving and when the agents are
sufciently risk-averse, social learning region is very small. It
can be interpreted as: agents having a strong risk-averse attitude
dont prefer to learn from the crowd; but rather face the same
consequences, when P (cid:9)= I.

1066

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

Fig. 2. The social learning region for the risk-aversion parameter   (0, 1].
It can be seen that the curves corresponding to 
do not intersect
and their separation (social learning region) varies with . Here P = I, i.e, the
value is a random variable.

and 

Fig. 4. The value function V () and the double threshold optimal policy
() are plotted over (2). The signicance of the double threshold policy is
that the stopping regions are non-convex. The implication of non-convex stop-
ping set for the market observer is that - if he believes that it is optimal to stop,
it need not be optimal to stop when his belief is larger.

multiple thresholds and the stopping region in general is non-
convex.

Example 1: Fig. 4 displays the value function and optimal

policy for a toy example having the following parameters:

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

B =

0.8 0.2
0.3 0.7

, P =

1

0

0.06 0.94

, c =

1
2
2.5 0.5

The parameters for the market observer are chosen as: d =
1.25, f = [0 3],  = 0.8 and  = 0.9.

From Fig. 4 it is clear that the market observer has a double
threshold policy and the value function is discontinuous. The
double threshold policy is unusual from a signal processing
point of view. Recall that (2) depicts the posterior probabil-
ity of no change. The market observer changes its mind -
it switches from no change to change as the posterior proba-
bility of change decreases! Thus the global decision (stop or
continue) is a non-monotone function of the posterior probabil-
ity obtained from local decisions in the agent based model. The
example illustrates the unusual behaviour of the social learning
lter.

C. Multi-state Markov chains

The structural results for the risk averse social learning lter,
namely Theorem 1 and Theorem 2, apply to multi-state Markov
chains. However, in numerical examples, to illustrate the opti-
mal policy, we have used 2-state Markov chains to model the
stock value. Multi-state Markov chain examples can also be
considered, but the numerical solution is substantially more
expensive and one has to resort to suboptimal methods such
as open loop feedback control (OLFC) [47] to compute a pol-
icy. In [17], structural results for the optimal policy in the risk
neutral case are considered. It is of interest to generalize these
results to the risk averse case considered in this paper.

V. DATASET EXAMPLE

Here, we illustrate multi-agent quickest change detection
by considering a dataset from the Tech Buzz Game, which
is a stock market simulation launched by Yahoo! Research

Fig. 3. The social learning region for the risk-aversion parameter   (0, 1]. It
can be seen that the social learning region is absent when agents are sufciently
risk-averse and is larger when the stock value is known to change, i.e, P (cid:6)= I.

B. Nonconvex Stopping Set for Market Shock Detection

We now illustrate the solution to the Bellmans stochas-
tic dynamic programming equation (17), which determines the
optimal policy for quickest market shock detection, by consid-
ering an agent based model with two states. Clearly the agents
(local decision makers) and market observer interact  the local
decisions ak taken by the agents determines the public belief
k and hence determines decision uk of the market observer
via (14).
From Theorem 2, the polytopes P 
3 are sub-
(2)),P 
sets of [0, 1]. Under (A1) and (A2), P 
2 =


(2), 1], where 
are the
[
belief states at which H (2, 1) = H (2, 2) and H (1, 1) =
H (1, 2) respectively. From Theorem 2 and (17), the value
function can be written as,

2 and P 
1 ,P 

3 = [0, 


(2)),P 

1 = [



(2), 

and 



V () = min{C(, 1), C(, 2) + V ()I(  P 
1 )

(cid:6)

V (T (, a))(, a)I(  P 
2 )

aA

+ 
3 )}
+ V ()I(  P 

The explicit dependence of the lter on the belief  results in
discontinuous value function. The optimal policy in general has

KRISHNAMURTHY AND BHATT: SEQUENTIAL DETECTION OF MARKET SHOCKS

1067

B. Simulation Model

The stock market simulation is modelled as shown in Fig. 5.
A stocks buzz score is an indicator of the number num-
ber of buzz searches over the past seven days, as a percentage
of all searches in the same market [49]. Thus, if searches for
the stock named SKYPE make up 80 percent of all Yahoo!
searches in the telecommunication application software market,
then SKYPEs buzz score is 80. The buzz scores of all technolo-
gies within a market always add up to 100. The scores reect
the ground truth, based on which the value of the stock is cal-
culated. The payout and dividend are directly proportional to
the buzz score. The value of a stock is a function of the pay-
out of the stock and its dividend [49]. The state xk is chosen
to represent value of the stock, with xk = 1 indicating a high
valued stock and xk = 2 indicating a low valued stock. At each
trading instant, the traders (or players) have access to the cur-
rent search buzz associated with each of the stocks measured
by the number of users searching information on it at Yahoo
Search. The noisy observations, yk, are chosen as the search
buzz which is a proxy for the popularity(sentiment) of the stock
[49]. The choice of probabilities for the observation matrix B
was motivated by the experimental evidence provided in [50],
that when there is social learning alone, the trading rate was
71% based on peer effects. Since the local decision likelihood
matrix R = B in the social learning region (in our model), the
parameters were chosen as

(cid:2)

(cid:3)

B =

0.7 0.3
0.3 0.7

The probabilities in the transition matrix P were chosen
to reect the time window considered. For tractability, it is
assumed that all agents have the same attitude towards risk, i.e,
same risk-aversion parameter  in (19).

Tech Buzz game has an order book indicating: trader id,
date and time of the transaction, the stock traded, number of
shares bought or sold, cost of the transaction, and price of
the stock before the transaction. The order book information
is available from the dataset. Individual agents choose to buy
(a = 1) or sell (a = 2) depending on the past history of actions,
price and search buzz to minimize their CVaR measure of trad-
ing. On each day the stock is traded, we consider only the
agent that buys or sells maximum shares and record its trad-
ing decision (positive or negative values in the dataset). This is
reasonable assumption since the agent trading maximum shares
(big players in nance) will signicantly inuence the public
belief.

The cost parameters in (5), were chosen to reect the intu-
ition that purchasing a high valued product at a high price will
maximize the utility5 (minimize the cost). The costs of all the
traders are assumed to be the same for simplicity. The costs for
the market observer, (12) and (13), were chosen to accommo-
date the desired trade-off between false alarm and delay penalty.
With these parameters, the market observer seeks to determine
if there is a change in the value of the underlying asset from the
actions of these agents using the public belief.

5With additional data regarding budget constraints, tools from expected
utility theory [51], [52] and revealed preferences [53], [54], [55], may be used
to estimate the parameters in the agents utility/cost function.

Fig. 5. Model of Tech-Buzz game. Daily Buzz Scores and the individual trad-
ing decisions of all the agents is available in the dataset. Value of the stock is
calculated using the method in [49]. In the simulated model, the public belief
is updated after every trading decision and the Market Observer announces a
change based on the optimal policy.

and OReilly Media on March 15, 2005 to gain insights into
forecasting high-tech events and trades. The overall setup is
described in Fig. 5.

A. Tech Buzz Game Pricing Mechanism

The Tech Buzz game uses Dynamic Pari-Mutuel Market
(DPM) as its trading mechanism. DPM was developed in [48]
as a mechanism for risk allocation and information speculation.
A DPM is a hybrid between pari-mutuel (i.e, redistributive -
guaranteed to pay out exactly the money taken in) and a contin-
uous double auction (CDA) market. DPM is designed to have
innite liquidity, like pari-mutuel markets, wherein the traders
can always purchase shares of any stock at any time at a price
automatically set by the mechanism. Like in CDA, DPM incor-
porates information arriving over time. A market using DPM
as its trading mechanism changes the price for the stock based
on the demand for the stock: price increases when the demand
increases. The price is set by the market using a price function
[48], which has the exibility to accommodate the properties
desired. Tech Buzz game consisted of multiple sub-markets
trading stocks of contemporary rival technologies. DPM played
the role of a market maker that accepts orders at its current price
and adjusts the price after each order. In the Tech Buzz game,
DPM set the price by equating the ratio of prices of any two
stocks (within the sub-market) by the ratio of number of shares
outstanding for the two stocks at any time of the market. So, if
q = [q1, q2, . . . , qn] is the vector of outstanding shares for the
n stocks in a sub-market, the price for stock i is given by the
price function dened by [49]:

pi(q) =

qi(cid:8)(cid:4)n

j=1 q2
j

i

(21)

where  > 0 is a free parameter. The traders buy or sell the
stocks depending on the price to maximize their utility. Tech
Buzz dataset was chosen to demonstrate the framework as
the individual actions for the duration of the game was made
available by Yahoo.

1068

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

Fig. 6. Quantized values of the stock SKYPE and the scaled buzz scores during
April - July is shown. It is seen that the value changed in the month of June,
2005. The market observers aim is to detect the change in the value, i.e, when
xk = 1, using only the trading decisions.

Fig. 7. The daily trading decisions of the agents is shown with the correspond-
ing belief update. Here a = 1 corresponds to buying and a = 2 corresponds to
selling the stock. Since (2)  [0, 0.354] is the stopping region, it corresponds
to (1)  0.646. The change was detected one day after it occurred.

C. Dataset and Quickest Detection

From the Yahoo! Dataset6, the buzz score for the stocks
SKYPE and IPOD trading in markets VOIP and PORTMEDIA
respectively, was obtained for the period from April 1, 2005 to
July 27, 2005. Quantized values of the stocks SKYPE and IPOD
during the period is shown in the Fig. 6 and Fig. 9 along with
the scaled buzz score.

1. Quickest Detection for SKYPE: From Fig. 6, it is seen
that the stock value changed during the month of June, 2005.
To apply the quickest detection protocol, we consider a window
from May 17, 2005 to June 8, 2005. It is observed that the price
was (almost) constant during this period with a value close to
13 per stock. The trading decisions (along with the value) and
the public belief during this period are shown in Fig. 7.

(cid:3)

(cid:2)

(cid:9)

(cid:10)

(cid:3)

The cost parameters chosen according to the rationale

described in Section V-B are:

(cid:2)

c =

0.5 1
1 0.5

, f =

0 2

, d = 0.8

The probabilities in the transition matrix P were chosen
to reect the time window considered. As E{0} = 25, the
transition matrix assuming a geometrically distributed change
time is:

P =

1

0

0.04 0.96

It was observed that the state changed on June 6, 2005 and
for a risk-aversion factor of  = 0.45, it was detected on June 7,
2005. The value function and the optimal policy for the market
observer are shown in Fig. 8. As seen from Fig. 8, the optimal
policy is a threshold. In general, however, the optimal policy in
a social learning model can be non-convex as shown in Fig. 4.
The stopping set corresponds to (2)  [0, 0.354]. The regions
(2)  [0, 0.34) and (2)  [0.76, 1] correspond to the regions
where social learning is absent. It can be observed that the
value function is discontinuous. This implies that small changes

6Yahoo! Webscope,

http://research.yahoo.com/Academic_Relations
ydata-yrbuzzgame-transactions-period1-v1.0, ydata-yrbuzzgame-buzzscores-
period1-v1.0

Fig. 8. Value function V () and the optimal policy () are plotted over
(2) for  = 0.45. Here 
correspond to the boundary points of
the social learning region. () = 1 corresponds to stop and () = 2
corresponds to continue. (2)  [0, 0.354] corresponds to the stopping region.

and 

in the public belief can result in large changes in the optimal
cost accrued by the market observer. This unusual feature is
again due to the social learning dynamics; in classical quickest
detection the optimal cost is a continuous function of the belief.
2. Quickest Detection for IPOD: From Fig. 9, it is seen
that the stock value changed during April and July. To apply
the quickest detection protocol, we consider a window from
July 2, 2005 to July 10, 2005. It is observed that the price was
(almost) constant during this period with a value close to 17
per stock. The trading decisions (along with the value) and the
public belief during this period are shown in Fig. 10.

The cost parameters chosen according to the rationale

described in Section V-B are:

(cid:2)

(cid:3)

(cid:9)

(cid:10)

c =

0.5 1
1 0.5

, f =

0 1.8

, d = 0.95

The probabilities in the transition matrix P were chosen
to reect the time window considered. As E{0} = 9, the

KRISHNAMURTHY AND BHATT: SEQUENTIAL DETECTION OF MARKET SHOCKS

1069

Fig. 9. Quantized values of the stock IPOD and the scaled buzz scores during
April - July is shown. It is seen that the stock value changed during April, 2005
and July, 2005. The market observers aim is to detect the change in the value,
i.e, when xk = 1, using only the trading decisions.

Fig. 10. The daily trading decisions of the agents is shown with the correspond-
ing belief update. Here a = 1 corresponds to buying and a = 2 corresponds to
selling the stock. Since (2)  [0, 0.368] is the stopping region, it corresponds
to (1)  0.632. The change was detected on the day it occurred with a higher
penalty on the delay.

transition matrix assuming a geometrically distributed change
time is:

(cid:2)

(cid:3)

P =

1

0

0.11 0.89

It was observed that the state changed on July 9, 2005 and
for a risk-aversion factor of  = 0.45, it was detected on July
9, 2005. It is seen that when the delay penalty is increased, the
change is detected on the same day. The value function and the
optimal policy for the market observer are shown in Fig. 11.
(2)  [0, 0.368] corresponds to the stopping set.

VI. CONCLUSION

The paper provided a Bayesian formulation of the problem
of quickest detection of change in the value of a stock using
the decisions of socially aware risk averse agents. From a sig-
nal processing point of view, the formulation and solutions
presented here are non-standard due to the three properties
described in Section I. The quickest detection problem was
shown to be non-trivial - the stopping region is in general

Fig. 11. Value function V () and the optimal policy () are plotted over
(2) for  = 0.45. Here 
correspond to the boundary points of
the social learning region. () = 1 corresponds to stop and () = 2
corresponds to continue. (2)  [0, 0.368] corresponds to the stopping region.

and 

non-convex when the agents risk attitude was accounted for
by considering a coherent risk measure, CVaR. Results which
characterize the structural properties of social learning under
the CVaR risk measure were provided and the importance of
these results in understanding the global behaviour was dis-
cussed. It was observed that the behaviour of these risk-averse
agents is, as expected, different from risk neutral agents. Risk
averse agents herd sooner and dont prefer to learn from the
crowd, i.e, social learning region is smaller the more risk-averse
the agents are. There is an opportunity to apply the framework
to study the behaviour of interacting agents in online prediction
markets such as Iowa Electronic Markets7, Trade Sports and
Foresight Exchange.

APPENDIX A

Denition 1 MLR Ordering [56] (r): Let 1, 2  (X)

PRELIMINARIES AND DEFINITIONS
be any two belief state vectors. Then 1 r 2 if

1(i)2(j)  2(i)1(j), i < j, i, j  {1, . . . , X}.

Denition 2 First-Order Stochastic Dominance (s): Let
1, 2  (X) be any two belief state vectors. Then 1 s 2
if

X(cid:6)

1(i)  X(cid:6)

i=j

i=j

2(i) for j  {1, . . . , X}.

(cid:2)

(cid:2)

2  v

Lemma 3: [56] 2 s 1 iff for all v  V, v
1,
where V denotes the space of X- dimensional vectors v, with
non-increasing components, i.e, v1  v2  . . . vX.
Lemma 4: [56] 2 s 1 iff for all v  V, v
1,
where V denotes the space of X- dimensional vectors v, with
non-decreasing components, i.e, v1  v2  . . . vX.
7The authors thank an anonymous reviewer for this suggestion.

(cid:2)

2  v

(cid:2)

1070

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016



X : 1(cid:2)

={  R

X  = 1, 0  (i)1 for all i  X}
Let (X)
Denition 3 Submodular function [46]: A function f :
(X)  {1, 2}  R is submodular if f (, u)  f (, u) 
f (, u)  f (, u), for u  u,  r .
Y  A  R satises a single crossing condition in (y, a) if

Denition 4 Single Crossing Condition [46]: A function g :

g(y, a)  g(y, a)  0  g(y, a)  g(y, a)  0

for a > a and y > y. For any such function g,



a

(y) = argmin

(22)
Theorem 5 [46] If f : (X)  {1, 2}  R is sub-modular,

g(y, a) is increasing in y.

a

then there exists a u

() = argmin

u{1,2}f (, u) satisfying,


 r   u



()  u



()

APPENDIX B

PROOFS

The following lemmas are required to prove Theorem 1 and
Theorem 2. The results will be proved for general state and
observation spaces having two actions.

Lemma 6: For a nite state and observation alphabet,
 Ey[max{(c(x, a)  z), 0}]} is equal to c(i, a)

{z + 1

argmin
zR
for some i  {1, 2, . . . , X}.

Proof: Let y be the belief update (p.m.f) with observa-
tion y, i.e, y(i) = Py(x = i). Let Fy(x) denote the cumulative
distribution function. For simplicity of notation, let hy(z) =
 Ey[max{(c(x, a)  z), 0}]. The extremum of hy(z) is
z + 1
attained where the derivative is zero. It is obtained as follows.

1

1


Ey[max{(c(x, a)  z), 0}]

h

lim
z0

hy(z) = z +
(cid:2)
y(z) = 1 +
Ey[max{c(x, a)zz, 0}]Ey[max{(c(x, a)z), 0}]
(cid:11)

1


z

Ey

max{c(x, a)zz, 0} max{(c(x, a)z), 0}
(cid:13)

0  I0>(c(x,a)z)  1  I(c(x,a)z)>0

(cid:14)

z

(cid:12)

= 1 +


lim
z0
1
= 1 +

= 1  1


Ey

Py(c(x, a) > z).

d
dz

(cid:2)(cid:2)
y (z) = 1


(Fy(z)) and therefore h

Also, h
have, argmin
zR

y (z)  0. We
(cid:2)(cid:2)
{hy(z)} = {z : Py(c(x, a) > z) = }. Since X
is a random variable, c(x, a) is a random variable with real-
izations c(i, a) for i  {1, . . . X}. Hence z = c(i, a) for some
i  {1, 2, . . . , X}.
(cid:2)
The result of Lemma 6 is similar to Proposition 8 in [57].
It was shown in [45] that y+1 r y. Also, MLR dominance
implies rst order dominance, i.e, y+1 s y.

Lemma 7: Let l and k be the indices such that

{hy(z)} = c(l, a)
{hy+1(z)} = c(k, a)

argmin

zR
argmin

zR

For all y  {1, 2 . . . , Y }, k  l.
Proof: Proof is by contradiction. From Lemma 6, we have
Fy(c(l, a)) = 1   and Fy+1(c(k, a)) = 1  . Suppose l >
k. We know that Fy+1(z) is a monotone function in z. Since
l > k, Fy+1(c(l, a)) > 1  . But, by denition of rst order
stochastic dominance, Fy(z)  Fy+1(z) for all z. Therefore,
Fy(c(l, a))  Fy+1(c(l, a)) > 1  , a contradiction.
(cid:2)

From Lemma 6 and equation (19), we have

l1(cid:6)
k1(cid:6)

1


i=1

i=1

H (y, 2) = c(l, 2) +

H (y + 1, 2) = c(k, 2) +

1


y(i)(c(i, 2)  c(l, 2)),

y+1(i)(c(i, 2)  c(k, 2))

Lemma 8: H (y, 2)  H (y + 1, 2) if   1  Py(x=X).
Proof: From the denitions of H (y, 2) and H (y +

l1(cid:6)

1, 2) we have,
H (y, 2)  H (y + 1, 2) = c(l, 2)  c(k, 2)
1

 c(l, 2)c(k, 2) +

y(i)(c(i, 2)  c(l, 2))+

y(i)(c(i, 2)c(l, 2))

l1(cid:6)

1


i=1

i=1

+

k1(cid:6)
y+1(i)(c(k, 2)  c(i, 2))

1


k1(cid:6)
y(i)(c(k, 2)  c(i, 2))

i=1

+

1


i=1

(23)

Equation (23) follows from Lemma 3 and can be simplied as

H (y, 2)  H (y + 1, 2)  c(l, 2)  c(k, 2)+
l1(cid:6)
y(i)(c(k, 2)  c(l, 2))+

k1(cid:6)
y(i)(c(k, 2)  c(i, 2))

1


i=1

i=l

1

 c(l, 2)  c(k, 2)  1


(cid:2)



y
where  is such that i = c(l, 2)  c(k, 2) for i = 1, . . . , l  1
and i = c(i, 2)  c(k, 2) for i = l, . . . k  1. Clearly, i  0
and decreasing. Right hand side of inequality attains its maxi-
mum when k = X and l = 1 and i = c(l, 2)  c(k, 2) for all
i. Therefore, we have
H (y, 2)  H (y + 1, 2)  c(l, 2)  c(k, 2)  1

 (c(l, 2)  c(k, 2))  1


(c(l, 2)  c(k, 2))(1  Py(x = X))

y



(cid:2)

After rearrangement we have,

H (y, 2)  H (y + 1, 2) 

  (1  Py(x = X))



(c(l, 2)  c(k, 2))

KRISHNAMURTHY AND BHATT: SEQUENTIAL DETECTION OF MARKET SHOCKS

1071

Since   1  Py(x = X) and (c(l, 2)  c(k, 2))  0 (follows
from Lemma 7 and assumption (A2)), we have H (y, 2) 
(cid:2)
H (y + 1, 2).

From Lemma 6 and (19), we have

H (y, 1) = c(l, 1) +

y(i)(c(i, 1)  c(l, 1)),

X(cid:6)
X(cid:6)

1


i=l+1

i=k+1

H (y + 1, 1) = c(k, 1) +

1


y+1(i)(c(i, 1)  c(k, 1))

Lemma 9: H (y + 1, 1)  H (y, 1)

if   1  Py+1

(x = X).

Proof: From the denitions of H (y + 1, 1)

H (y, 1) we have,

and

H (y + 1, 1)  H (y, 1) = c(k, 1)  c(l, 1)+

1


i=k+1

X(cid:6)
y+1(i)(c(i, 1)  c(k, 1))
X(cid:6)

y(i)(c(i, 1)  c(l, 1))

 1

 c(k, 1)  c(l, 1)+
X(cid:6)
y+1(i)(c(i, 1)  c(k, 1))
1
X(cid:6)


i=k+1

i=l+1

y+1(i)(c(i, 1)  c(l, 1))

 1


i=l+1

(24)

Equation (24) follows from Lemma 4 and can be simplied as

Since   1  Py+1(x = X) and c(k, 1)  c(l, 1)  0 (fol-
lows from Lemma 7 and assumption (A2)), we have H (y +
1, 1)  H (y, 1).
(cid:2)
function

Lemma 10: Let   (1  Py(x = X)). The
H (y, a) satises the single crossing condition i.e,
(H (y, 1)  H (y, 2))  0  (H (y + 1, 1)  H (y + 1, 2))  0

Proof: Assume (H (y, 1)  H (y, 2))  0. We have,

H (y, 1)  H (y, 2)  0
 H (y, 1)  H (y + 1, 2)  0

(25)

Equation (25) follows from Lemma 8. And,

H (y, 1)  H (y + 1, 2)  0
 H (y + 1, 1)  H (y + 1, 2)  0

(26)
(cid:2)
Lemma 10 is a crucial result which helps us to prove

Equation (26) follows from Lemma 9.

Theorem 1 and Theorem 2.

Proof of Theorem 1: From Lemma 10, H (y, a) satises
the single crossing condition and hence is sub-modular in
(, y) = argminH (y, a)
(y, a). Using Theorem 5, we get a
is increasing in y.

Proof of Theorem 2: From Lemma 10, H (y, a) satises
the single crossing condition. It is easily veried that the belief
states satisfy the following property



{ : H (y, 1)  H (y, 2)  0} 
{ : H (y + 1, 1)  H (y + 1, 2)  0}

that

says

Equation (27)

(27)
the curves { : H (y, 1) 
H (y, 2) = 0} for all y  Y do not intersect. Also from (18)
and (27), it is easily veried that there are at most Y + 1
local decision likelihood matrices R (can be less than Y +
1 when H (y, 1)  H (y, 2) > 0 for some y  Y, for all ).
The matrices R from (18) and Theorem 1 are constant on each
of the Y + 1 polytopes.

H (y + 1, 1)  H (y, 1)  c(k, 1)  c(l, 1)+
X(cid:6)
y+1(i)(c(i, 1)  c(k, 1))
1
X(cid:6)


i=k+1

y+1(i)(c(i, 1)  c(l, 1))

 1


i=l+1

REFERENCES

 c(k, 1)  c(l, 1)  1


(cid:2)


y+1

where  is such that i = c(i, 1)  c(l, 1) for i = l, . . . , k
and i = c(k, 1)  c(l, 1) for i = k + 1, . . . X. Clearly, i 
0 and decreasing. Right hand side of inequality attains its max-
imum when k = X and l = 1 and i = c(k, 1)  c(l, 1) for all
i. Therefore, we have

H (y + 1, 1)  H (y, 1)  (c(k, 1)  c(l, 1))

(c(k, 1)  c(l, 1))(1  Py+1(x = X))

 1


After rearrangement we have,

H (y + 1, 1)  H (y, 1)    (1  Py+1(x = X))



(c(k, 1)  c(l, 1))

[1] B. LeBaron, Agent-based computational nance, in Handbook of

Computational Eeconomics, 2006, vol. 2, pp. 11871233.

[2] B. LeBaron, Agent-based computational nance: Suggested readings
and early research, J. Econ. Dyn. Control, vol. 24, no. 5, pp. 679702,
2000.

[3] E. Samanidou, E. Zschischang, D. Stauffer, and T. Lux, Agent-based
models of nancial markets, Rep. Prog. Phys., vol. 70, no. 3, p. 409,
2007.

[4] V. Al, M. Cristelli, L. Pietronero, and A. Zaccaria, Minimal agent based
model for nancial markets I, Eur. Phys. J. B, vol. 67, no. 3, pp. 385397,
2009.

[5] L. Tesfatsion and K. L. Judd, Handbook of Computational Economics:
Agent-Based Computational Economics. Amsterdam, The Netherlands:
Elsevier, 2006, vol. 2.

[6] R. Cont and J.-P. Bouchaud, Herd behavior and aggregate uctuations
in nancial markets, Macroeconomic Dyn., vol. 4, no. 2, pp. 170196,
2000.

[7] C. Avery and P. Zemsky, Multidimensional uncertainty and herd behav-
ior in nancial markets, Amer. Econ. Rev., vol. 88, no. 4, pp. 724748,
Sep., 1998.

[8] A. Park and H. Sabourian, Herding and contrarian behavior in nancial

markets, Econometrica, vol. 79, no. 4, pp. 9731026, 2011.

1072

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

[9] C. Chamley, Rational Herds: Economic Models of Social Learning.

Cambridge, U.K.: Cambridge Univ. Press, 2004.

[10] R. Cont, Volatility clustering in nancial markets: Empirical facts and
agent-based models, in Long Memory in Economics. New York, NY,
USA: Springer, 2007, pp. 289309.

[11] T. Lux and M. Marchesi, Volatility clustering in nancial markets:
A microsimulation of interacting agents, Int. J. Theor. Appl. Finance,
vol. 3, no. 4, pp. 675702, 2000.

[12] D. Challet et al., Minority Games: Interacting Agents in Financial

Markets. London, U.K.: Oxford Univ. Press, 2013.

[13] M. Cristelli, L. Pietronero, and A. Zaccaria, Critical overview of agent-

based models for economics, arXiv preprint arXiv:1101.1847, 2011.

[14] N. Apergis and S. M. Miller, Do structural oil-market shocks affect stock

prices? Energy Econ., vol. 31, no. 4, pp. 569575, 2009.

[15] S. Gilchrist, V. Yankov, and E. Zakrajsek, Credit market shocks and eco-
nomic uctuations: Evidence from corporate bond and stock markets,
J. Monetary Econ., vol. 56, no. 4, pp. 471493, 2009.

[16] S. Das and M. Magdon-Ismail, Adapting to a market shock: Optimal
sequential market-making, in Proc. Adv. Neural Inf. Process. Syst., 2009,
pp. 361368.

[17] V. Krishnamurthy, Quickest detection POMDPs with social learning:
Interaction of local and global decision makers, IEEE Trans. Inf. Theory,
vol. 58, no. 8, pp. 55635587, Aug. 2012.

[18] P. Artzner, F. Delbaen, J.-M. Eber, and D. Heath, Coherent measures of

risk, Math. Finance, vol. 9, no. 3, pp. 203228, 1999.

[19] S. Bikhchandani, D. Hirshleifer, and I. Welch, A theory of fads, fash-
ion, custom, and cultural change as informational cascades, J. Political
Economy, vol. 100, no. 5, pp. 9921026, Oct. 1992.

[20] I. Welch, Sequential sales, learning, and cascades, J. Finance, vol. 47,

no. 2, pp. 695732, Jun. 1992.

[21] A. V. Banerjee, A simple model of herd behavior, Quart. J. Econ.,

vol. 107, no. 3, pp. 797817, Aug. 1992.

[22] L. R. Glosten, Insider trading, liquidity, and the role of the monopolist

specialist, J. Business, vol. 62, no. 2, pp. 211235, Apr. 1989.

[39] A. N. Akansu and M. U. Torun, A Primer for Financial Engineering:
Financial Signal Processing and Electronic Trading. New York, NY,
USA: Academic, 2015.

[40] M. Avellaneda and S. Stoikov, High-frequency trading in a limit order

book, Quant. Finance, vol. 8, no. 3, pp. 217224, 2008.

[41] V. Krishnamurthy and A. Aryan, Quickest detection of market shocks in
agent based models of the order book, in Proc. IEEE 51st Conf. Decision
Control (CDC), Dec. 2012, pp. 14801485.

[42] M. F. Neuts, Structured Stochastic Matrices of MG-1 Type and Their

Applications. New York, NY, USA: Marcel Dekker, 1989.

[43] V. Krishnamurthy, Bayesian sequential detection with phase-distributed
change time and nonlinear penaltyA POMDP lattice programming
approach, IEEE Trans. Inf. Theory, vol. 57, no. 10, pp. 70967124, Oct.
2011.

[44] S. Karlin and Y. Rinott, Classes of orderings of measures and related
correlation inequalities: I. Multivariate totally positive distributions,
J. Multivariate Anal., vol. 10, no. 4, pp. 467498, 1980.

[45] W. S. Lovejoy, Some monotonicity results for partially observed Markov

decision processes, Oper. Res., vol. 35, no. 5, pp. 736743, 1987.

[46] D. M. Topkis, Supermodularity and Complementarity. Princeton, NJ,

USA: Princeton Univ. Press, 1998.

[47] D. P. Bertsekas, Dynamic Programming and Optimal Control. Belmont,

MA, USA: Athena Scientic, 1995, vol. 1.

[48] D. M. Pennock, A dynamic pari-mutuel market for hedging, wager-
ing, and information aggregation, in Proc. 5th ACM Conf. Electron.
Commerce, 2004, pp. 170179.

[49] Y. Chen, D. M. Pennock, and T. Kasturi, An empirical study of dynamic
pari-mutuel markets: Evidence from the tech buzz game, in Proc. Web
Mining Web Usage Anal. Workshop (WebDKK), Las Vegas, NV, USA,
2008.

[50] L. Bursztyn, F. Ederer, B. Ferman, and N. Yuchtman, Understanding
mechanisms underlying peer effects: Evidence from a eld experiment
on nancial decisions, Econometrica, vol. 82, pp. 12731301, 2014.

[51] D. A. Graham, Estimating the state dependent utility function, Nat.

[23] A. N. Shiryaev and A. Aries, Optimal Stopping Rules. New York, NY,

Resources J., vol. 23, p. 649, 1983.

USA: Springer, 2007, vol. 8.

[24] H. V. Poor and O. Hadjiliadis, Quickest Detection. Cambridge, U.K.:

Cambridge Univ. Press, 2009, vol. 40.

[25] M. Frisn, Optimal sequential surveillance for nance, public health, and

other areas, Sequential Anal., vol. 28, no. 3, pp. 310337, 2009.

[26] R. A. Cohn, W. G. Lewellen, R. C. Lease, and G. G. Schlarbaum,
Individual investor risk aversion and investment portfolio composition,
J. Finance, vol. 30, no. 2, pp. 605620, May 1975.

[27] B. Donkers and A. V. Soest, Subjective measures of household pref-
erences and nancial decisions, J. Econ. Psychol., vol. 20, no. 6,
pp. 613642, 1999.

[28] S. Mitra and T. Ji, Risk measures in quantitative nance, Int. J. Bus.

Continuity Risk Manage., vol. 1, no. 2, pp. 125135, 2010.

[29] R. T. Rockafellar and S. Uryasev, Optimization of conditional value-at-

risk, J. Risk, vol. 2, pp. 2141, 2000.

[30] P. Krokhmal, J. Palmquist, and S. Uryasev, Portfolio optimization with
conditional value-at-risk objective and constraints, J. Risk, vol. 4, no. 2,
pp. 4368, 2002.

[31] C. Lim, H. D. Sherali, and S. Uryasev, Portfolio optimization by
minimizing conditional value-at-risk via nondifferentiable optimization,
Comput. Optim. Appl., vol. 46, no. 3, pp. 391415, 2010.

[32] F. Andersson, H. Mausser, D. Rosen, and S. Uryasev, Credit risk
optimization with conditional value-at-risk criterion, Math. Program.,
vol. 89, no. 2, pp. 273291, 2001.

[33] Y. Feng, F. Rubio, and D. Palomar, Optimal order execution for algorith-
mic trading: A CVaR approach, in Proc. IEEE 13th Int. Workshop Signal
Process. Adv. Wireless Commun. (SPAWC), Jun. 2012, pp. 480484.

[34] R. Forsythe, T. A. Rietz, and T. W. Ross, Wishes, expectations and
actions: a survey on price formation in election stock markets, Journal
of Economic Behavior & Organization, vol. 39, no. 1, pp. 83110, 1999.
[35] C. R. Plott and K. Y. Chen, Information aggregation mechanisms:
Concept, design and implementation for a sales forecasting problem,
California Inst. Technol., Division of the Humanities and Social Sciences,
Working Papers 1131, 2002.

[36] R. H. Thaler and W. T. Ziemba, Parimutuel betting markets: Racetracks

and lotteries, J. Econ. Perspect., vol. 2, no. 2, pp. 161174, 1988.

[37] J. M. Gandar, W. H. Dare, C. R. Brown, and R. A. Zuber, Informed
traders and price variations in the betting market for professional basket-
ball games, J. Finance, vol. 53, no. 1, pp. 385401, Feb. 1998.

[38] A. Cartea and S. Jaimungal, Modelling asset prices for algorithmic and
high-frequency trading, Applied Mathematical Finance, vol. 20, no. 6,
pp. 512547, 2013.

[52] W. N. Evans and W. K. Viscusi, Estimation of state-dependent utility
functions using survey data, Rev. Econ. Statist., vol. 73, no. 1, pp. 94
104, 1991.

[53] S. N. Afriat, The construction of utility functions from expenditure

data, Int. Econ. Rev., vol. 8, no. 1, pp. 6777, 1967.

[54] H. R. Varian, Intermediate Microeconomics: A Modern Approach. New

York, NY, USA: Norton, 2014.

[55] G. Charness, U. Gneezy, and A. Imas, Experimental methods: Eliciting

risk preferences, J. Econ. Beh. Organ., vol. 87, pp. 4351, 2013.

[56] A. Mller and D. Stoyan, Comparison Methods for Stochastic Models

and Risks. Hoboken, NJ, USA: Wiley, 2002, vol. 389.

[57] R. T. Rockafellar and S. Uryasev, Conditional value-at-risk for general
loss distributions, J. Banking Finance, vol. 26, no. 7, pp. 14431471,
2002.

Vikram Krishnamurthy (F05) received the Ph.D.
degree from the Australian National University,
Canberra, ACT, Australia,
in 1992. He is cur-
rently a Professor and holds the Canada Research
Chair
in stastistical signal processing with the
Department of Electrical Engineering, University
of British Columbia, Vancouver, BC, Canada. His
research interests include stastistical signal pro-
cessing and stochastic control with applications in
social networks, and dynamical models for protein
molecules in biosensing devices. He has served as a
Distinguished Lecturer for the IEEE Signal Processing Society and the Editor-
in-Chief of the IEEE JOURNAL SELECTED TOPICS IN SIGNAL PROCESSING.
He received a Honorary Doctorate from KTH (Royal Institute of Technology),
Stockholm, Sweden, in 2013. He is author of the book: Partially Observed
Markov Decision Processes (Cambridge University Press, 2016).

Sujay Bhatt received the M.Tech. degree from Indian
Institute of Technology Bombay, Mumbai, India. He
is currently working with Vikram Krishnamurthy at
the University of British Columbia, Vancouver, BC,
Canada. His research interests include social learning,
Bayesian learning over networks, and game theory.

