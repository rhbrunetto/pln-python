994

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

Filtering of a Discrete-Time HMM-Driven

Multivariate Ornstein-Uhlenbeck Model With

Application to Forecasting Market Liquidity Regimes

Anton Tenyakov, Rogemar Mamon, Member, IEEE, and Matt Davison

AbstractThis paper investigates the modeling of risk due to
market and funding liquidity by capturing the joint dynamics
of three time series: the treasury-Eurodollar spread, the VIX,
and a metric derived from the S&P 500 spread. We propose a
two-regime mean-reverting model for explaining the behaviour of
three time series, which mirror liquidity levels for nancial mar-
kets. An expectation-maximisation algorithm in conjunction with
multivariate lters is employed to construct optimal parameter
estimates of the proposed model. The selection of the modeling
set-up is justied by balancing the best-t criterion and model
complexity. The model performance is demonstrated on historical
market data, and a descriptive analysis of the different liquidity
measures shows the presence of clear high and low states.

Index TermsOrnstein-Uhlenbeck process, Markov chain,
change of measure, multivariate HMM ltering, TED, VIX, S&P
500, nancial distress.

I. INTRODUCTION

T HE purchase or sale of an asset like a stock, a bond, a

currency, or a traded commodity sometimes affects the
market price of that asset. The purchase of the asset tends to
drive the price up, the sale driving the pricing down. If large
quantities of an asset can be traded with a small impact on its
price, the market for that asset is said to be liquid. The liquid-
ity of markets can, however, change over time: during periods
of great nancial stress, markets may become unbalanced with
many more sellers than buyers making it hard to sell large quan-
tities of the asset without offering buyers a large discount to the
current market price.

Typically, cash and the short-term government debt of major
countries are very liquid assets which businesses and traders
can use to meet their immediate nancial needs. Although
major currencies are usually liquid assets, even they can

Manuscript received October 16, 2015; revised February 17, 2016 and March
26, 2016; accepted March 29, 2016. Date of publication March 31, 2016;
date of current version August 12, 2016. This work was supported in part by
the NSERC under Grant RGPIN/341780-2012 to R. Mamon and in part by
MITACS, Inc. and MPRIME Network, Inc. through the project Modelling
Trading and Risk in the Market led by M. Davison. The guest editor coor-
dinating the review of this manuscript and approving it for publication was
Dr. Dmitry M. Malioutov.

R. Mamon is with the Department of Statistical and Actuarial Sciences,
University of Western Ontario, London, ON N6A 5B7, Canada (e-mail:
rmamon@stats.uwo.ca).

A. Tenyakov is with the Treasury and Balance Sheet Management

Department, TD Bank Financial Group, Toronto, ON M5K 1A2, Canada.

M. Davison is with the Department of Statistical and Actuarial Sciences,
Department of Applied Mathematics, University of Western Ontario, London,
ON N6A 5B7, Canada.

Color versions of one or more of the gures in this paper are available online

at http://ieeexplore.ieee.org.

Digital Object Identier 10.1109/JSTSP.2016.2549499

at times suffer from severe illiquidity when they must be
exchanged in the foreign exchange market. For example, some
economists worry that the US dollar and US dollar-linked assets
could become much less liquid than at present if countries, such
as China, which hold trillions of dollars in dollar-denominated
US government bonds, began to dump them. This worry about
the impact of Chinese actions on US market liquidity is moti-
vated by past events. For example, Chinas support of a rising
yuan in July 2005 seemed to trigger the subsequent slide in US
Treasury bonds, with follow-on impacts on US mortgage and
corporate debt markets [26].

Businesses, and in particular nancial institutions, are also
said to possess sufcient liquidity if they can sell sufcient
assets to be able to repay counterparties. The recent Basel III
accord, which provides the risk management rulebook for
large international nancial institutions, focuses on the impor-
tance of liquidity and prescribes liquidity guidelines for assets
held by large nancial institutions. Basel III directives also
require the diversication of counterparty risk and imposes the
need for stress tests which could identify unusual market liq-
uidity conditions. The goal of these regulations is to prevent
undue weight in investments that are particularly susceptible to
liquidity shifts; see [3].

In 2007, the world was deemed to have experienced the worst
nancial crisis since the 1930s. This crisis originated in the
United States and spread across the global nancial markets
within less than a year. Some big nancial organisations and
banks declared bankruptcy. The downfall of Lehman Brothers
was the most calamitous high-prole default of this crisis; see
Gorton [22]. Whilst many nancial market events in 20072008
were considered to be a direct consequence of improper credit
risk management, it is also believed that the main trigger of
economic turmoil was the inability to predict liquidity in the
markets. In 2008, AIG had a huge portfolio of CDS and CDO
that was originally rated AAA but backed by subprime loans.
As a result of nancial instability, the AIG products were down-
graded and the company had to post additional collateral for its
positions. These events are believed to be the main trigger of the
liquidity crisis that began in September 2008, essentially bring-
ing AIG to a level of bankruptcy but eventually bailed out by
the US government.

In this paper, we propose a method of quantifying and
forecasting illiquidity in the nancial market. As nancial
turbulence cannot be avoided, warning systems that aid the
prediction of economic crunches are necessary to prepare mar-
ket participants to deal with future instability. We use three

1932-4553  2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.

See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

TENYAKOV et al.: FILTERING OF DISCRETE-TIME HMM-DRIVEN MULTIVARIATE ORNSTEIN-UHLENBECK MODEL

995

proxies to measure liquidity. These are the T-Bill Eurodollar
spread (TED spread), the Volatility Index (VIX), and a mea-
sure based on the difference between bid and ask prices. Each
of these indices measures, as well as the liquidity of the mar-
ket, something else, which is why we use all three to infer the
underlying market state. It is noted in Boudt et al. [6] that the
T-billEurodollar (TED) spread is directly correlated with mar-
ket stability. The TED spread is calculated as the difference
between the interest rate linked to interbank loans and the yield
on short-term US T-bills. Currently, its computation makes use
of the three-month London Interbank Offer Rate (LIBOR) and
the three-month T-bill yield rates. An increasing TED spread
usually portends a stock market meltdown as it is taken as a
sign of liquidity withdrawal. The TED spread, as described in
Boudt et al. [6], can gauge perceived liquidity risk in the general
economy since T-bills are risk-free instruments and the funding
liquidity risk of lending to commercial banks is encapsulated
by LIBOR. The TED spread does also measure market expec-
tations about credit risk, in addition to liquidity concerns. A
rising TED spread indicates that lenders view the default coun-
terparty risk to be rising as well. Thus, lenders either require
a higher rate of interest or settle for lower returns on safer
instruments such as T-bills. Conversely, when the default risk
of banks decreases, the TED spread falls; see Krugman [28].

We made contributions in terms of methodology in modelling
the regime switches in a multivariate OU model and under-
standing the dynamics of liquidity. We provide economic and
econometric motivations throughout this work by explicating
the relevance of chosen data for market illiquidity, and the pro-
posed model captures the joint effects of the 3 state processes.
This study demonstrates a useful application of signal process-
ing methods and technologies and serves the signal processing
community to be exposed to the state of the art in nancial
signal processing.

We aim to use hidden Markov models (HMMs) driving a
mean-reverting process in the analysis of the joint movements
of important economic indicators to forecast liquidity and illiq-
uidity states of the nancial market. The HMM ltering in
this paper uses the measure-change approach, which avoids the
forward-backward algorithm usually embedded in several l-
tering techniques, and hence our approach requires much less
memory during computation. Furthermore, other lters (e.g.,
Hamilton-type lters) are computationally intensive to imple-
ment because they are based on static algorithms that entail
full reruns involving the original data set every time a few data
points are newly added. In our case, we do not have to deal
with the issue of recalculation using the old data set that gets
larger as time evolves. Instead, we have dynamic online l-
ters that easily provide updates on various quantities related
to the Markov chain thereby updating the parameter estimates
quickly upon the arrival of new data. In this paper, we utilise
observed TED spread data as market signals and lter out
the state of the economy and subsequently liquidity levels.
Filtering results could be useful in assessing near-future mar-
ket stability. The proposed idea is very similar to that of Abiad
[1] wherein a regime-switching approach is used as an early
warning device in identifying and characterising periods of cur-
rency crises. It must be recognised, however, that a noticeable

TED spread movement cannot be taken as a pure indication
of extreme illiquidity/market downturns caused by severe illiq-
uidity. Whilst uctuations in the TED spread may happen
due to some signicant underlying factors, these uctuations
are sometimes caused by pure noise alone. In the late 1990s,
with the world battling the dot-com bubble and other nancial
upheavals, more instability and uncertainty in the behaviour of
the TED spread was observed.

The second indicator for liquidity levels that we consider
is the VIX. This is a trademarked ticker symbol for the
Chicago Board Options Exchange (CBOE)s market volatility
index and measures the implied volatility of S&P 500 index
options (www.cboe.com/micro/VIX). Whilst the VIX also cap-
tures market sentiment about the future degree of volatility and
market uncertainty, this is wrapped together with liquidity con-
cerns, as in an illiquid market prices will move more for a given
level of trading. Using historical data, VIX appears to capture
some periods of illiquidity that were not picked up by the TED
spread.

The third indicator we consider is a metric based on the evo-
lution of the S&P 500. At the end of October 2012, market illiq-
uidity was felt to be brought about by cautious trading as spec-
ulators and traders anxiously anticipated the result of the US
presidential election. This illiquidity was captured by an S&P
500-based bid-ask spread metric but not by the TED spread.
The absolute level of the bid-ask spread is connected with mar-
ket microstructure issues, but the changes in the level reect,
at least in part, liquidity concerns. This fact can be explained
by the absence of direct causation effect between the proposed
metric and the TED spread. For this reason, for a reason-
ably adequate study modelling liquidity can be accomplished
by investigating the TED spread dynamics along with other
indicators such as the VIX and an S&P 500-driven measure.

There have been many attempts to model and explain illiq-
uidity such as those put forward in van der End and Tabbae
[34], Macini et al. [30], and Vayanos et al. [35], amongst others.
Whilst these proposed modelling approaches include Monte
Carlo simulations to demonstrate their implementability, they
are nonetheless built on simple assumptions for tractability and
do not offer the capacity for dynamic calibration using market
data. This leaves a huge gap between model implementation
which unies theoretical approaches and real data. In this work,
we attempt to address this gap by explaining how to t the
model with the data. With the aid of ltered estimates, we pro-
vide a description of the data dynamics with emphasis on the
effect of illiquidity shocks.

In forecasting illiquidity, we use a discrete-time Markov
chain assumed to modulate the parameters of a mean-reverting
process so that several economic regimes can be embedded
into the model. As mentioned in Brunnermeier [7], the econ-
omy has a liquidity self-stabilising effect, which is either a
loss or a margin type; see Brunnermeier and Pedersen [8]
for additional discussion. It is, therefore, reasonable to look at
the Ornstein-Uhlenbeck (OU) process as a simple model for the
TED spread and thus liquidity level in general, as self stabilis-
ing and the mean-reverting properties of a process are closely
linked. More specically, a market downturn with a falling spi-
ral effect could elicit a re sale amongst borrowers, which in

996

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

turn decreases prices and further worsen funding conditions.
Clearly, such downturn effects are associated with episodes of
deation and periods of poor economic growth. As such, the
evolution of indicators for these observed economic events can
be captured by an OU process that exhibits temporary low and
high value levels.

Goyenko [23] showed a strong correlation between TED and
VIX as major indicators in illiquidity estimation; in the same
paper, a measure for the evaluation of stock illiquidity was
also presented using market bid and ask prices. Our work is
based on similar assumptions, but instead of nding correla-
tion between the major indicators of illiquidity, we incorporate
as much information as possible into our model by simultane-
ously using the three market variables integrated by a set of
multidimensional dynamic lters. For stock illiquidity, the S&P
500-based spread is used. The main consideration of this paper
is the prediction of illiquidity level based on previous informa-
tion contained in a joint time series of indicators. The dynamic
ltering algorithms structure enables the nding of expected
state probability of the illiquidity level at the next time steps.

Our main contribution is the development of an HMM-driven
model tailored for capturing and predicting liquidity risk lev-
els. The models t and forecasting power are examined using
historical data series. Detailed empirical implementation proce-
dures are provided along with the discussion of various aspects
concerning model validation and other post-diagnostic mod-
elling considerations. Our numerical results demonstrate that
the proposed model has satisfactory capacity in identifying
periods of liquidity crises. This modelling tool shows promise
in aiding the prediction of economic crunches occurring over a
short-time horizon.

The paper is organised in the following way. Section II gives
an overview of the modelling set up including the HMM formu-
lation and introduction of the change of measure concept for the
ltering technique. A description of the mathematical ltering
equations is also presented. We specify in section III the data
used for the numerical estimation and prediction experiments.
The process of recursive parameter calculation together with
the discussion of the econometric interpretation of the dynam-
ics of estimates are delineated in section IV. Finally, section V
concludes.

II. MODELLING SETUP

An Ornstein-Uhlenbeck (OU) process rt is any process that

satises the stochastic differential equation (SDE)

drt = (  rt)dt + dWt,

(1)

where Wt is a standard Brownian motion dened on some prob-
ability space (,F, P ), and ,  and  are positive constants.
The parameter  is the mean level to which the process tends to
move to over time, whilst  is the speed of mean reversion, and
 is the volatility. Assuming that ,  and  are constants, the
solution of (1) by Itos lemma is given by

rt = r0e

t + (1  e

t) + e

t

t

0

esdWs,

(2)

(cid:2)

where r0 is the initial value at time t = 0. Applying the prop-
erty of a Gaussian distribution and the Itos isometry to nd the
variance of rt in (2), we nd that

Var[rt] = 2 1  e

2t
2

(3)

with t = tk+1  tk.
So, following the results established in James and Webber

[27], the discretisation of the analytic solution in (2) is

rtk+1

= rtk e

(t) +

(cid:4)
(cid:3)
(1  e2t)k+1,

1  et



(cid:5)

1
2

+ 

(4)
where the equality means in distribution and k+1  N (0, 1).
Invoking Hamilton [24] (pp. 5356) further, a discrete version
of an OU process also has the mean-reverting property.

In the sequel, it is assumed that ,  and  will be time-
dependent; hence, we respectively denote them by t, t and t.
To capture the switching of economic regimes, we also assume
that the values of parameters t, t and t are modulated by a
discrete-time Markov chain with a nite-state space. We regard
the state of the underlying Markov chain as the regime of an
economy (e.g., see Zhou and Mamon [38]), or more specif-
ically a liquidity regime dependent on major factors causing
economic turbulence. The scenario when t is high charac-
terises the worse regime, i.e., it corresponds to very unstable
periods of the global nancial crisis. In particular, there are two
cases: (i) a high t with high t occurs when t reaches a high
value with t considerably spiking up creating a completely
unstable behaviour for t, and (ii) a high t and a low t is also
possible when correlation is used as a liquidity proxy. This is
because all equity correlation go to 1 in case of a crisis, lead-
ing to a high t and low t. See further Campbell et al. [10],
and Longin and Solnik [29] for studies on correlation during a
crisis.

(g)

A distinct contribution of this paper is the detailed implemen-
tation of parameter estimation under a multivariate OU setting
which extends the one-dimensional framework of Erlwein and
Mamon [19]. We consider d OU processes; each process is
t with component g  {1, , . . . , d}. All vec-
denoted by r
tors and matrices are written in bold lowercase and uppercase
letters, respectively. Following the idea developed by Elliott
et al. [16] let us assume that (,F, P ) is a probability space
under which xk is a homogeneous Markov chain with a nite-
state space in discrete time. Thus, xk evolves according to the
equation

xk+1 = xk + vk+1,

(5)

where  is a transition matrix and vk+1 is a martingale incre-
ment, i.e., E[vk+1|Ck] = 0, where Ck = Fk  Rk. For ease of
calculation, the state space of xk is associated with the canon-
ical basis of IRN , which is the set of unit vectors eh with eh
having 1 in its hth entry and 0 elsewhere, h = 1, 2, . . . , N.
Here, Fk = {x0, x1, . . . , xk} is the ltration generated by
x0, x1, . . . , xk and Rk is the ltration generated by the {rk}
process.

TENYAKOV et al.: FILTERING OF DISCRETE-TIME HMM-DRIVEN MULTIVARIATE ORNSTEIN-UHLENBECK MODEL

997

N(cid:14)

i=1

i=1

i=1

i=1

k

(13)

(cid:16)

= E

(cid:5)ck, ei(cid:6) =

(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)Ck

= E[k|Ck].

(cid:5)xk, eh(cid:6) = 1. Thus,

(cid:5)xk, ei(cid:6)
ck(cid:17)
Consequently, equation (13) implies that
i=1(cid:5)ck, eh(cid:6) .

by the Bayes theorem for conditional expectation. Let ck =
N(cid:14)
N(cid:14)
E[kxk|Ck] and note that
(cid:15)
(cid:5) E[kxk|Ck], ei(cid:6)
N(cid:14)
(cid:13)pk =
k+1(cid:14)
k+1(cid:14)
k+1(cid:14)

Similar to Erlwein et al. [21] or Erlwein and Mamon [19], we

(cid:5)xn1, ej(cid:6)(cid:5)xn, es(cid:6)

dene the following quantities:

Oj
k+1x =

J js
k+1x =

(cid:5)xn, ej(cid:6)

(14)

(15)

n=1

n=1

N

(cid:5)xn1, ej(cid:6)f (rn) , 1  j  N.

T j
k+1(f )x =

(16)

n=1

Equations (14) and (15) are the respective number of jumps
from es to ej and the amount of time that x occupies the state
ej up to k + 1 . The quantity T j
k+1(f ) is an auxiliary process
that depends on the function f of the observation process; in our
case, f takes the form f (r) = r, f (r) = r2 or f (r) = rk+1rk.
Other than generalising the framework in Erlwein and
Mamon [19], our contribution includes expressing recursive l-
tering equations compactly though matrix notation. This allows
efcient computation and decreases parameter estimation time
using vector-optimised mathematical packages (e.g., MATLAB
by The Mathworks). Dene the diagonal matrix D(rk) with
elements dij by

(cid:9)

(cid:10)

d(cid:8)

g=1



(dij(rk)) =

(11)

g
k

=



1
(g)
i

exp
(cid:2)

= rk

(g)
k,i

where 
0 otherwise.

 1
2

(

(g)
k,i

2  r2
(cid:3)

k

)

,

+

(g)
rk1
i
(g)
i



(g)
i

for i = j

(17)

For any process Gk, we denote the conditional expec-
tation, under P , of kGk by (G)k := E[kGk|Ck]. We
provide recursive lters for ck, (J j,ix)k, (Oix)k and
(T i(f )(g)x)k.
Theorem 1: Let D be the matrix dened in (17). Then

We shall make the parameters of equation (4) regime-
switching so that each component of the d-dimensional obser-
vation process can be written as

(g)
k+1 = r
r

(g)
k e

(g)(xk)t + (1  e

(g)(xk)t)(g)(xk)

(cid:6)

+ (g)(xk)

1  e2(g)(xk)t

2(g)(xk)



(g)
k+1,

(6)

)(cid:2)

(g)
N

(g)
1 , 

)(cid:2)
(g)
(g)
2 , . . . , 
N

(g)
2 , . . . , 
(g)
1 , 

(g)
1 , 
and (g) = (

(g)
, (g) = (
where (g) = (
2 , . . . ,
)(cid:2)
(g)
are all in IRN . Thus,

N
given the state-space representation, we have in equation (6),
k , xk(cid:6), (g)(xk) = (cid:5)(g)
(g)(xk) = (cid:5)(g)
k , xk(cid:6) and (g)(xk) =
(cid:5)(g)
k , xk(cid:6), where (cid:5),(cid:6) is the usual scalar product and (cid:7)
denotes the transpose of a vector.
Equation (6) can be re-expressed as

(g)
k+1 = (g)(xk)r
r
1  g  d,

(g)
k

+  (g)(xk) + (g)(xk)

(g)
k+1,

(7)

where 
random variables and

(2)
k , . . . , 

(1)
k , 

(d)
k

are independent standard Gaussian

(g)(xk)t,

(g)(xk) = e
 (g)(xk) = (1  e

(g)(xk) = (g)(xk)

(cid:6)
(g)(xk)t)(g)(xk),
1  e2(g)(xk)t

(8)

(9)

.

(10)

2(g)(xk)

The succeeding calculations are inspired by the approach
described in Elliott et al. [16], where lters are derived under
some equivalent probability measure P . Under this ideal mea-
sure, the observations are independent and identically dis-
tributed random variables making the calculations of condi-
tional expectations easy. The lters, which are conditional
expectations, are then related back to the real-world by the
use of Bayes theorem for conditional expectation. Following
Elliott et al. ([16] chapter 3.4, page 62), the ideal measure P is
equivalent to the real-world measure P via the Radon-Nikodym
derivative constructed as

d(cid:8)

g=1

=

K(cid:8)
(cid:9)

k=1



(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)CK
rk (cid:11)

1

K = dP
d P

where

g
k

=

(g)
k



=

exp

(g)(xk1)

(g)

k , K  1, 0  1,
(cid:10)

 1
2

(

(g)
k

2  r2

k

)

(cid:12)

,

k

Write the conditional probability of xk given Ck under P as
(cid:13)pi
:= P (xk = eh|Ck) = E[(cid:5)xk, eh(cid:6)|Ck],
where(cid:13)pk = ((cid:13)p1
k,(cid:13)p2
k, . . . ,(cid:13)pN
(cid:13)pk = E[xk|Ck] =

)(cid:2)  IRN . Now,
E[kxk|Ck]
E[k|Ck]

k

rk1(g)(xk1) +  (g)(xk1)

(g)(xk1)

.

(12)

ck = Dck1

(J j,ix)k = D(rk)(J j,ix)k1

+ (cid:5)ck1, ei(cid:6)(cid:5)D(rk)ei, ei(cid:6)jiej

(Oix)k = D(rk)(Oix)k1

+ (cid:5)ck1, ei(cid:6)(cid:5)D(rk)ei, ei(cid:6)ei
(T i(f )(g)x)k = D(rk)(T i(f )(g)x)k1

+ (cid:5)ck1, ei(cid:6)(cid:5)D(rk)ei, ei(cid:6)f (r

(g)
k

)ei.

(18)

(19)

(20)

(21)

998

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

Proof: The proof follows similar derivations of the lter-
ing equations as those provided in Elliott et al. [16], Erlwein,
(cid:2)
et al. [21] or Erlwein and Mamon [19].
To obtain the model parameter estimates, we use the
Expectation-Maximisation (EM) algorithm [14]. The EM esti-
mation for the multi-regime setting is very similar to that in the
one-dimensional case illustrated in Erlwein and Mamon [19],
so the proof of the next theorem is omitted.

As indicated in the above discussion, the model parameters
(g), (g) and (g) have estimates that depend on the lters
of quantities given in Theorem 1. These dynamic parameter
estimates are given as follows.

Fig. 1. Plot of TED spread recorded on 11th or last trading day of the month.

k

k



T i

(cid:12)(cid:12)

k

(g)
i

=

T i

r(g)

. . . r

(g)
K



T i

r(g)

(g)
(g)
1 , r
2

J j,i

 (Oi)

(cid:11)
(cid:22)
(cid:11)
(cid:11)

(cid:12)
(cid:22)
(cid:11)
(cid:11)

(g)
(g)
r
k+1, r
k


(cid:12)(cid:12)

T i

(cid:11)
(cid:12)(cid:12)
(cid:11)

(g)
i 

(cid:12)(cid:12)
(cid:11)

k
T i

 
k
(r(g))2
(g)
i 

(cid:13)ji =
(cid:13)
(cid:13)
(cid:13)

Theorem 2: If a multivariate data set with row compo-
1  g  d is drawn from the model
nents r
described in equation (7) then the EM parameter estimates are
(cid:23)(cid:23)
(cid:11)
(cid:11)
(cid:13)
(cid:12)(cid:12)
+ ((cid:13)
(cid:11)
(cid:11)
(cid:12)
(cid:12)(cid:12)
(cid:23)(cid:23)
(cid:11)

(cid:11)
(cid:11)
(cid:12)(cid:12)

)2
Oi
(r(g))2

(cid:22)

k
(g)
(g)
r
k+1, r
k

(g)
i
T i
(g)
i 

(cid:11)T i

+(cid:13)

(cid:22)
(cid:11)

k+1
T i


T i

(r(g))2

 (Oi)

k

 (Oi)

k



T i

(r(g))2

(g)
i

)2

r(g)

(cid:11)

+



 2

(g)
i

=

(r(g))2

k

(cid:12)(cid:12)

k

(g)
i

=

r(g)

k+1

(cid:12)(cid:12)

(22)

(23)

(24)

(cid:12)(cid:12)

(g)
i 

k

(cid:11)

T i

(cid:11)

k+1

k

((cid:13)
(cid:11)
(cid:13)
(cid:13)

(cid:11)

(cid:13)

k

 2

(g)
i

r(g)

(g)
i 

T i
 (Oi)

k

k

.

(25)

Proof: The derivations of (22)(25), which generalise the
lters for the univariate OU case, are straightforward based on
(cid:2)
Erlwein and Mamon [19].

Remarks:
1) To implement

the recursive equations in Theorem 1
in providing the dynamic updating of the estimates
(22)(25) under Theorem 2, note that (H i)k =
(H i(cid:5)1, xk(cid:6)) = (cid:5)1, (H ixk)(cid:6),
for some function H
which may denote J, O or T .

2) Equation (23) in Theorem 2 contains the parameter  (g),
which must be known prior to achieving a workable recur-
sion. In practice, the sequence of equations (23) and (24)

can be implemented in reverse order. That is,(cid:13) (g) can be

estimated using the previous knowledge of (g). This lat-
ter implementation was adopted in the empirical part of
this paper, resulting in signicant stability in parameter
estimates.

III. DESCRIPTION OF DATA FOR IMPLEMENTATION

To model the levels of liquidity, we use three monthly time
series data covering the period of 30 April 199830 April

2013; the data points are recorded at the last trading day of
each month. These data sets are: (i) TED spread obtained from
Bloomberg, (ii) S&P 500 VIX compiled by the CBOE, and
(iii) calculated average spread of S&P 500 based on the data
collected by Bloomberg. The indicator in (iii), MktIll (a short
form for Market Illiquidity), was adopted from Goyenko [23]
and dened by

MktIll = 2 Bid  Ask
Bid + Ask

,

(26)

where Bid and Ask are the respective bid and ask prices.

The choice of the end-of-month time series data sets in
our analysis is mainly due to convenience as they are read-
ily available from all data sources. To ensure that we are not
missing possible anomalous patterns in the data, we also look
at the time series values recorded at other days of the month.
Figure 1 shows the dynamics of the TED spread for the data
collected on the last day of the month (TED-30) and on the
11th of each month (TED-11); if the 11th is not a trading day
we utilise the value of the previous trading day. Except for
a few time points that correspond to the recession period of
the late 90s and 200709 crisis, the behaviour of the two time
series is almost identical. Although not shown here, the graph
of VIX and Mktill data series display a very similar pattern.
The main purpose of this research is not to accurately predict
the dynamics of individual variables, but to capture the joint
effects of these variables to forecast illiquidity. Whilst we use
monthly discretisation, our method works for any discretisation
frequency. The important consideration is to select a discretisa-
tion grid (monthly in our case) just ne enough to capture all
the major economic breaks and instabilities, and without creat-
ing distortions or introducing extra noise in the data. Data sets
with weekly and quarterly frequency were also considered. The
general behaviour of the data remains the same, and therefore
monthly observations are appropriate for our intended applica-
tion given the correct number of calculations involved in the
window processing of data points.
The data set for our ltering applications is formed by con-
structing a matrix with a dimension of 181  3 over the period
30 April 1998  30 April 2013 with the TED, VIX and MktIll in
the rst, second and third columns, respectively. Figure 2 dis-
plays a visualisation of the movements of the TED spread, VIX
and MktIll  100 variables. Note that we use MktIll  100 to
scale the magnitude of MktIll and make it comparable to that
of the TED and VIX. The instability of the TED spread in

TENYAKOV et al.: FILTERING OF DISCRETE-TIME HMM-DRIVEN MULTIVARIATE ORNSTEIN-UHLENBECK MODEL

999

metric mentioned in Ceulemans and Kiers [11] may be utilised.
These criteria are independent of the nature of the data; they are
general tools that can be applied to any data set with an ultimate
goal of selecting the model that optimally balances goodness of
t and model complexity. To make the discussion meaningful
and the mathematics tractable, one may posit that the econ-
omy can only have two states - a crisis regime associated
with abnormally high indicator values and a regular regime.
A transitional state may be created and persists over some time
due to the weighted combination of volatilities under the above
two regimes.

Brokers who have long-term positions in different securities
hedge their portfolios differently depending whether to expect
a future crash or rally. In the trading world the value of the
nancial contract can only go up, go down or stay the same.
In general, it is assumed that every stock always has a positive
growth, i.e., it earns more money than what one can get from
a risk-neutral investment. Therefore, even when the potential
growth on a stock, mutual fund and other risky investment port-
folios is minimal but the level of liquidity is high, the economy
is still deemed to be in the good (or high) state. Consequently,
when the percentage change in the value of the index or any
other major indicators of the nancial state of the country (GDP,
for example) is relatively close to the risk-neutral rate, we
regard the economy to be in the high state. Furthermore, we
rely on the results of Boudt et al. [6] and Dionne [15] advanc-
ing two-state models in investigating liquidity. Any three-state
model will be shown later as a special case of the two-regime
framework, where the third state is between the high and
low states.

Finding parameter estimates via the likelihood maximisation
procedure is a tedious endeavour but such a procedure provides
the best results for dynamic modelling, if it can be accom-
plished. We shall use the rst 40 points of the multidimensional
data set to calculate the starting parameters for our lters. For
simplicity, when obtaining initial lter values, it is assumed at
the outset that the set of true parameters

 = {ij, (g)(xi),  (g)(xi), (g)(xi)}

is homogeneous, i.e., the values of the set  do not change
when subsets of the data are chosen in a sense that true param-
eters of the model stay the same for data subsets of any type
or size. Whilst this may be a strong assumption, the lters will
eventually adapt and parameters will change accordingly as the
number of algorithm passes is increased. The likelihood func-
tion, conditional on knowing which state the process xi is in, is
given by

d(cid:8)

g=1

(cid:24)
K(cid:8)
(cid:26)

i=1

(cid:25)

 exp

 (r

L =

or

(cid:27)(cid:28)

  (g)(xi))2

(g)
i

1

2((g)(xi))2
i+1  (g)(xi)r
(g)
K(cid:8)

d(cid:8)

2((g)(xi))2

L =

g,i,

g=1

i=1

(27)

(28)

Fig. 2. Plot of TED spread, VIX and MktIll  100.

the late 1990s - early 2000s has been limited to the informa-
tion technology bubble, political dispute surrounding the 2000
US presidential election, political and nancial crises in post-
Soviet Russia, and a recession in Japan; see [4] and [33], for
example. The dot-com price bubble persisted through 1997-
2000 climaxing in March 2000. It is worth noting that these
three indicators pin down the occurrence of the nancial market
instability directly affecting liquidity. However, each indicator
captures this instability at different moments and with differ-
ent durations. The superiority of one measure over the others is
therefore not clear. This is usually the case whenever the dura-
tion of the market crash is short and rapid economic recovery is
expected by many.

On the other hand, the subprime mortgage crisis in 2008-
2009 was captured by all three measures at once. The noticeably
unusual spike in the TED spread during June 2007 seems to
clearly herald the coming of extreme nancial meltdown that
happened in August-September 2008. From the plot of the
trivariate series, we also observe the cyclical behaviour of the
economy shifting from stable to unstable states. This provides
support for using a multivariate version of the OU process in
modelling the generating process of the underlying data.

IV. NUMERICAL APPLICATION

A. Calculation of Estimates and Other
Assumptions

Implementation

Several approaches may be employed to nd initial param-
eters for ltering algorithms. These include the methods in
Erlwein and Mamon [19], Erlwein et al. [20], and Date and
Ponamareva [12], amongst others. Good starting parameter val-
ues are necessary to stabilise the ltering algorithm procedure.
However, estimating initial parameters is not straightforward
considering the nature of the data and other factors. Whilst none
of the initial-value estimation algorithms must be disregarded,
the choice of which to adopt mainly depends on (i) achiev-
ing stable performance and (ii) relative ease of implementation.
In this paper, we combine the above-mentioned approaches to
generate reasonable initial estimates.

To choose the number of states in a regime-switching model,
statistical inference-based methods such as the Akaike cri-
terion information (AIC)
[2], Bayes information criterion
(BIC) described in Schwarz [31] and Hardy [25], or the CHull

1000

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

INITIAL PARAMETER ESTIMATES FOR THE FILTERING ALGORITHMS

INITIAL PARAMETER ESTIMATES FOR THE FILTERING ALGORITHMS

UNDER THE TWO-STATE SETTING

UNDER THE ONE-STATE SETTING

TABLE I

TABLE II

where

(cid:26)

g,i = 

(g)

i+1  (g)(xi)r

r

(g)
i

(g)(xi)

(cid:27)

  (g)(xi)

(29)

and  stands for
distribution.

the density of

the standard Gaussian

As the sequence of the states xi is hidden, a recursive algo-
rithm similar to the one proposed in Hardy [25] is used. The
idea of the algorithm is to calculate the most probable set  by
building the likelihood function using recursions and to apply
standard computer routines for maximisation of the function
over a desired set of parameters. Although Hardys method was
designed for the geometric Brownian motion model, we extend
it in a straightforward manner to handle our multidimensional
data set assumed to follow the OU process. Additionally, the
denition of the log-likelihood function is extended by using
the sum of the log-likelihood functions for the TED, VIX and
MktIll data sets. Adopting the notation of this article, the den-
sity function of the process at time t in Hardys algorithm, given
the whole set of parameters including the state of the Markov
chain at time t, is changed to g,i(ri+1, ri, ).

The results of the initial parameter estimation under the two-
state model are provided in Table I. The values of parameters
1 and 2, encapsulating the speed of mean reversion, can be
considered to be almost the same for all three variables. This
empirical result that these parameters have uniformly close
estimated values is no coincident and gives additional strong
support to the hypothesis about the dependency of TED, VIX
and MktIll on the same underlying factor.

The HMM ltering algorithms can only give a local maxi-
mum, and at times could be extremely unstable to implement.
Such limitation can be rectied by choosing initial estimates
that t the data very well and working in double precision arith-
metic. It is also possible to employ a symbolic package such as
Wolfram Researches Mathematica, but in that case the speed
of the computation drops dramatically. The static log-likelihood
maximisation approach appears to yield initial parameters that
afford appreciable stability for our OU-based lters.

The starting values for the one-regime model are obtained
by simply maximising the likelihood function (27), taking into
account that x = 1, i.e., the system always operates under one
state. The results of this optimisation are exhibited in Table II.
As expected, the initial parameter values for the single state
model lie between the corresponding estimates for the two-
regime model. The only parameter which does not follow this
observation is ; but even then such  values corresponding

to the three indicators produce a stable convergence for the
ltering procedure outlined in the next subsection.

B. Filtering Procedure

In the estimation of the parameters of the underlying OU pro-
cess, we employ the method described in section II using the
starting parameters in Tables I and II. The data set described
in section III contains columns consisting of g = 1 (TED), 2
(VIX), and 3 (MktIll). There are 141 time points considered in
our ltering application. The rst 40 time points for the three
vectors of data are used for the initialisation discussed in sub-
section IV-A. The predictive power of the model is tested on the
last 60 monthly observations from the middle of the nancial
crisis (30 May 2008) up to the end of the time series data (30
April 2013). All results are analysed and evaluated using a com-
bination of both intuitive and rigorous statistical approaches for
decision making.

The dynamics of the estimates ,  and  are computed by
rst producing the estimates of ,  and . Then using equations
(8), (9) and (10), we back out the values of the desired model
parameters. The OU lters described in section II were imple-
mented with a moving window spanning vectors of data which
extends the idea of the procedure in Erlwein and Mamon [19].
More specically, vectors of data are processed through the
recursive equations of the lter to obtain the best estimates (in
the sense of conditional expectation) after several time points.
Once the parameters are estimated from a batch of vector of
data points, they become starting values for the next recursion,
and so on. The size of the processing window is determined
by likelihood-maximum or other statistical criterion. Owing to
the complex nature of the data and the ltering equations, we
employ the smallest window possible (3 points per window
in our case) that gives stability to the algorithms. Whilst this
choice results in a relatively high volatility, the outputs contain
ample information about parameter uctuations.

Figures 36 provide output for the parameter estimates of the
OU process corresponding to the TED spread. An implication
that can be drawn from the behaviour of transition probabili-
ties in Figure 6 is that, over our study window, major illiquidity
events do not happen very often, but when they do, they do
not last very long and are not severe until the nancial market
collapse in 20072008; see Figures 36 or 10. After the cri-
sis in 2008, the structure of the economy changed completely.
This fact is supported by Figure 3, wherein the mean-reverting
levels are switched. This phenomenon occasionally arise in
similar ltering applications (e.g., Xi and Mamon [36] or Xi
and Mamon [37]). In our case, this anomaly can be explained by
the presence of higher volatility levels during times of greater
uncertainty. This is substantiated by Figure 5 as the level of 
reaches the highest level in 2008.

TENYAKOV et al.: FILTERING OF DISCRETE-TIME HMM-DRIVEN MULTIVARIATE ORNSTEIN-UHLENBECK MODEL

1001

Fig. 3. Evolution of the mean-level estimates for the TED spread data.

Fig. 6. Evolution of the ltered transition probabilities obtained from the
multivariate data.

Fig. 4. Evolution of the speed of mean reversion for the TED spread data.

Fig. 5. Evolution of the volatility levels for the TED spread data.

The other odd behaviour shown by the ltering results is the
negativity of  in Figure 4. Even though the formulation of the
OU process does not allow the parameter  to be negative, the
multi-regime construction of OU process, proposed by Elliott
and Wilson [18], does not restrict  to be always positive. From
an empirical perspective, the somewhat bizarre result of   0

Fig. 7. Evolution of the mean-reverting level under the one-state setting using
the TED spread data.

is justied by the fact that the OU process becomes unstable
as can be seen during the 20072008 period when there was
a sudden unfolding of several related nancial and economic
events leading to the crisis. The negative value estimated for
the speed of mean-reversion is, perhaps, a signal of the failure
of the Ornstein-Uhlenbeck framework to capture what is going
on during periods of extreme market stress. However, during
stable periods, the speed of mean reversion remains positive and
the process tends to a mean level as time goes by.

The results of the dynamic parameter estimation based on
ltering under the one-regime framework are illustrated in
Figures 79. The dynamics of the parameters look similar to
those of the two-regime model. This fact can be interpreted
as an excellent t of the 2-state HMM-modulated OU model
to the data set. We use the AIC and BIC tailored to several
previous works on ltering, in particular, Date et al. [13] and
Xi and Mamon [37] to show that the proposed two-regime
model provides a better explanation of the data compared to the
one-regime setting. The AIC and BIC metrics are computed as

AIC = ln L  p

(30)

1002

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

we report that it takes 5-6 algorithm steps for the OU lters to
adjust and maintain some stability.

C. Filtering and Forecasting Illiquidity

There are many approaches in modeling illiquidity based on
the TED spread or other major economic factors. However,
these approaches either look at a certain threshold of the TED
spread as benchmark for illiquidity (see, Boudt et al. [6] and
Krugman [28]) or correlate the TED spread with another major
economic variable (see Goyenko [23]).

In this work, we introduce a new approach which is natu-
rally suited for dynamic ltering algorithms. It relies on the

dynamics of(cid:13)pk = E[xk|Ck]. As previously specied, the two-
(cid:13)pk(1) = (cid:5)(cid:13)p, e1(cid:6) (cid:11) 0.5, i.e., the probability of being in regime
fore easy to buy and sell every contract. But, if(cid:13)pk(1) (cid:12) 0.5,
which is equivalent to (cid:13)pk(2) = (cid:5)(cid:13)p, e2(cid:6) (cid:11) 0.5, then the mar-

regime model is instructive in that each regime corresponds to
illiquid and liquid states of the market. We put forward that if

1 is very high, the market is extremely liquid and it is there-

Fig. 8. Evolution of the speed of mean-reversion under the one-state setting
using the TED spread data.

ket is very illiquid, which typically corresponds to recession
or period of economic crisis brought about by some major
nancial events.

We implement the above approach with the objective of
determining regimes of illiquidity through the HMM lter-
ing of data. The output of this implementation is displayed in
Figure 10; the upper panel shows the joint plot for the evolving
estimates of i, i, ij and pk(1) whilst the lower panel shows
the TED evolution plotted on the same time scale. We omit the
graphs of the two other indicators (VIX and MktIll) to avoid
overcrowding the lower panel of this illustration. They are dis-
played in Figure 2 and similar demonstrations can be produced
using various combination of results in Figures 26. Two salient
points are as follows. First, every point estimate of each model
parameter was obtained via a data processing procedure based
on a deemed optimal ltering window of size three. Clearly,
this inherent dependence on the data processing window size is
a source of variability for modelling results. Second, from the
side-by-side comparison in Figure 10, we delineate four major
trigger events of the 20072008 liquidity crisis captured by the
model with the period of their occurrences charted against the
time axis. These events are the hedge fund crash, total loss
underestimation by the federal regulators in the US, major col-
lapse of the economy in September 2008 and recovery stage.
One important nding of our modelling implementation is the
pre-crisis stage that is captured only by the illiquidity state pro-
cess, but not by the dynamics of any of the model parameters.
Thus, it is essential to monitor all parameters, indicators and
any other metrics simultaneously.

It is asserted in Brunnermeier [7] that the hedge fund crash
was one of the major triggers of the 20072008 liquidity col-
lapse. The outcome obtained in our model is consistent with
Brunnermeiers argument; in fact, our results indicate that the
hedge fund crash event triggered the chain of events leading
to the deepening of the nancial crisis up to January-February
2009. The severity of the crisis is seemingly marked by the
underestimation of the total loss by federal regulators. After
this event, return to the normal liquidity level did not happen

Fig. 9. Evolution of the volatility under the one-state setting using the TED
spread data.

TABLE III

COMPARISON OF SELECTION CRITERIA FOR SINGLE- AND

2-STATE REGIME MODELS

and

BIC = ln L  1

2 p ln N,

(31)

where L is the log-likelihood function for the entire multivariate
data set, N is the number of observations and p is the num-
ber of parameters in a model. With the calculated value for
the log-likelihood function of the last 141 vector of monthly
observations, both the AIC and BIC signies that the two-
regime model signicant outperforms the one-state model; see
Table III.

The general trend of the behaviour of the data during the cri-
sis in 20072008 is captured by the model. Nonetheless, due
to extreme volatility movements, getting a perfect t during
this period is a challenge. Given the initial parameter estimates,

TENYAKOV et al.: FILTERING OF DISCRETE-TIME HMM-DRIVEN MULTIVARIATE ORNSTEIN-UHLENBECK MODEL

1003

Fig. 11. Evolution of the estimated liquidity-state probabilities and one-step
ahead forecasts of liquidity-state probabilities.

forecasting of liquidity state. First,(cid:13)pk is computed for some k,
and second, the expectation of(cid:13)pk+1 given Ck is calculated as

E[(cid:13)pk+1|Ck] = k(cid:13)pk,

(32)

where k is dened by equation (5).

E[(cid:13)pk(1)|Ck1] and shows the estimates of(cid:13)pk(1) obtained by

Figure 11 depicts the dynamics of the prediction values

To distinguish the liquid from the illiquid state, we pro-

applying the ltering algorithms on the last 60 points of the data
set. The drastic change in the movement of estimated probabil-
ities between the fourth and fth time points corresponds to a
signicant drop in the values of all the three variables (TED,
VIX, MktIll) in April-May 2009; such a twist was perfectly

captured by the dynamics of(cid:13)pk(1).
pose a criterion that hinges on (cid:13)pk(i) (cid:11) 0.5 for i = 1, 2. If
(cid:13)pk(1) > 0.6, it is assumed that there is enough evidence to
the higher the(cid:13)pk(1), the higher the liquidity level. Whenever
(cid:13)pk(1) < 0.4, the nancial markets are assumed illiquid, and
state when 0.4 <(cid:13)pk(i) < 0.6, i = 1, 2 is not an easy endeav-

conclude that the level of liquidity in the market is high, and
traders can take positions with little or no probability of acquir-
ing additional risks due to market or funding liquidity. So,

therefore, additional capital must be infused to deal with the
nancial distress.

Based on empirical evidence, typifying exactly the liquidity

our. This situation is characterised by a very high level of
uncertainty regarding market directions over a short time. On
the one hand, the state of uncertainty signals the occurrence
of future hard times. On the other hand, it can also be viewed as
a sign that economic stability is forthcoming after an economic
downturn. The case in point here is the period of early 2009,
when regulators used all possible schemes to stabilise the mar-
ket sentiments and provided instant articial liquidity to help
markets function the way they were intended to be. We note that
our proposed model gives somewhat overoptimistic estimates

for E[(cid:13)pk(1)] during the last period of the market crash in 2008,
the(cid:13)pk(1) still remain at the 0.4 level, coinciding with what was

and this requires some adjustment. However, the estimates for

Fig. 10. Side-by-side comparison between behaviour of model parameter esti-
mates and movement of the TED spread along with the identication of major
nancial market events through time.

without the painful experience of several major defaults in the
nancial industry and new drastic measures taken by the reg-
ulators. The proposed model and modelling approach capture
these dynamics exceptionally well. In winter 2009, after many
bankruptcies and government bailouts, trading levels started
to pick up. Subsequently, nancial uncertainty decreased, and
trading volume rose from already low level comparable to
that of the pre-crisis stage. Around this time, the consider-
able restructuring of the general economy caused the behaviour
of the variables we are examining to change altogether with
levels, movements and model parameter estimates completely
different from those of the past.

As previously noted, we also aim to predict the state or
regime of liquidity. We describe our technique to achieve the

1004

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 10, NO. 6, SEPTEMBER 2016

previously argued concerning state of uncertainty and arti-
cial liquidity. Liquidity state prediction for the last 48 data
points is accurate in the sense that the predictions jibe very well
with the classication of the liquidity state estimates.

The state of uncertainty can be viewed either as a third
regime in a two-state model, which is interpreted as the low-
est/worst bound for the high regime and upper/best bound for
the low regime. This can be explained from an econometric
point of view. Recession and upturn times in the economy are
generally followed by short periods of market anxiety. During
these unstable periods, liquidity can rise and fall quite fre-
quently because speculators do not have stable expectations
for the long-term horizons and short-term government interven-
tions can provide only temporary relief. It is rather difcult to
capture that state as it has in a way the characteristics of either
regime. Of course, the stability of a possible separate three-
regime model must be investigated as well, and could certainly
be an alternative model. Nevertheless, preliminarily results in
our case reveal that recursive algorithms do not provide even
an approximate convergence for nding the starting parame-
ters for the three-state model. Thus, we rule out the dynamic
three-regime setting as inappropriate for this data set.

V. CONCLUDING REMARKS

In this work, we developed an HMM-based modelling
approach in assessing levels of market and funding liquid-
ity risks. The structure of the proposed model incorporates
major econometric assumptions concerning factors of eco-
nomic recovery. We provided a detailed methodology on how
to extract information from major economic indicators, and
linking these to the short-term prediction of market illiquidity
or liquidity. The methodology employed made use of newly
developed multivariate HMM recursive ltering algorithms
expressed in matrix representations. Effects of mean-reversion
and liquidity state dependency were also explored.

The models implementability and forecasting performance
were investigated using market data. Results were analysed
against statistical metrics and interpreted by examining under-
lying historical nancial events. We found that the one-regime
model signicantly underperforms compared to a two-regime
model. Undoubtedly, a simple OU process cannot capture all
the very complex features of the liquidity risk in the nan-
cial market. A technique for liquidity-state estimation naturally
consistent with dynamic HMM ltering algorithms was put
forward and its validity was evaluated using past data.

An improvement that could be done with our modelling
approach is the further examination of the two-state model.
Its predictability of liquidity becomes uncertain if the condi-
tional probability of the Markov chain falls in the range [0.4,
0.6]. Despite our empirical and economic reasoning to support
our assumption and conclusions under this scenario, additional
analysis of this particular aspect is a promising research direc-
tion. Our preliminary results suggest that the three-state model
cannot be tted given the data we examined. There is however
a possibility that the three-regime model may work by adding
some other economic variables portraying clear multi-regime
behaviour.

Our suggested modelling construction and empirical work
used monthly data. Building on our results, further analysis
of data with different frequency could be carried out to open
avenues for modelling methods and insights about liquidity risk
over a long or very short-time periods. These entail establishing
new drivers, factors and determinants of liquidity to be included
in the ltering experiments. The HMM-driven OU process may
have to be tweaked to accommodate these new inputs leading
to new lters.

We put forward and empirically tested a new way of esti-
mating and predicting liquidity levels in the nancial market.
This approach provides a quantitative methodology that sup-
ports economic interpretation of our liquidity proxy variables.
The liquid/illiquid regimes pinpointed by our regime-switching
modelling approach accurately correspond to those identied
by practitioners. This paper therefore addressed the missing
link between economical and mathematical modelling of liq-
uidity. The methodology it contains could be useful for traders,
economists, regulators and policy makers.

The current recommended modelling and estimation set up
can be effectively exploited under sophisticated trading-scheme
environments. For example, underlying variables involved in
trading, valuation or reserve calculation for nancial derivative
contracts, are known to follow the OU process. Our ltering
equations can be employed to provide dynamic parameter esti-
mates both for pricing and risk management. Regulators may
also consider this model to study the impact of different con-
straints on the economy. Finally, we recognise, of course, the
papers limitation. That is, despite the promising features of our
proposed approach, there is the associated model risk with the
given parametric set up.

ACKNOWLEDGEMENTS

All authors sincerely appreciate the helpful comments from
three anonymous referees that signicantly improved this
manuscript.

REFERENCES

[1] A. Abiad, Early warning systems for currency crises: A regime-
switching approach, in Hidden Markov Models in Finance, R. Mamon
and R. Elliott, Eds. New York, NY, USA: Springer, 2007, pp. 155184.

[2] H. Akaike, A new look at the statistical model identication, IEEE

Trans. Autom. Control, vol. AC-19, no. 6, pp. 716723, Dec. 1974.

[3] Bank of International Settlement. (2010). Group of Governors and Heads
of Supervision Announces Higher Global Minimum Capital Standards
[Online]. Available: http://www.bis.org/press/p100912.pdf

[4] R. Bianchi, M. Drew, and T. Wijeratne, Systemic risk, the TED spread
and hedge fund returns, Int. J. Bus. Econ., vol. 1, no. 1, pp. 5978, 2009.
[5] N. Bloom and M. Floetotto, Good news at last? The recession will
be over sooner than you think, Centre for Economic Policy Research,
London, U.K., 2009 [Online]. Available: http://www.voxeu.org/article/
good-news-last-recession-will-be-over-sooner-you-think

[6] K. Boudt, E. Paulus, and R. Rosenthal, Funding liquidity, market liq-
uidity and TED spread: A two-regime model, Working Paper, 2010
[Online]. Available: http://dx.doi.org/10.2139/ssrn.1668635

[7] M. Brunnermeier, Deciphering the liquidity and credit crunch 2007

2008, J. Econ. Perspect., vol. 23, no. 1, pp. 77100, 2009.

[8] M. Brunnermeier and L. Pedersen, Market liquidity and funding liquid-

ity, Rev. Financial Stud., vol. 22, no. 6, pp. 22012238, 2008.

[9] K. Bulteel, T. Wilderjans, F. Tuerlinckx, and E. Ceulemans, CHull as an
alternative to AIC and BIC in the context of mixtures of factor analyzers,
Behav. Res. Methods, vol. 45, pp. 782791, 2013.

TENYAKOV et al.: FILTERING OF DISCRETE-TIME HMM-DRIVEN MULTIVARIATE ORNSTEIN-UHLENBECK MODEL

1005

[10] R. Campbell, K. Koedijk, and P. Kofman, Increased correlation in bear

[33] R. Sipley, Market Indicators: The Best-Kept Secret to More Effective

markets, Financial Anal. J., vol. 58, no. 1, pp. 8794, 2002.

[11] E. Ceulemans and H. Kiers, Selecting among three-mode principal com-
ponent models of different types and complexities: A numerical convex
hull based method, Brit. J. Math. Stat. Psychol., vol. 59, pp. 133150,
2006.

[12] P. Date and K. Ponomareva, Linear and nonlinear ltering in mathemat-
ical nance: A review, IMA J. Manage. Math., vol. 22, pp. 195211,
2011.

[13] P. Date, R. Mamon, and A. Tenyakov, Filtering and forecasting com-
modity futures prices under an HMM framework, Energy Econ., vol. 40,
pp. 10011013, 2013.

[14] A. Dempster, N. Laird, and D. Rubin, Maximum likelihood from incom-
plete data via the EM algorithm, J. Roy. Stat. Soc. B, Methodol., vol. 39,
no. 1, pp. 138, 1977.

[15] G. Dionne and O. M. Chun, Default and liquidity regimes in the bond
market during the 2002-2012 period, Can. J. Econ., vol. 46, no. 4,
pp. 11601195, 2013.

[16] R. Elliott, L. Aggoun, and J. Moore, Hidden Markov Models: Estimation

and Control. New York, NY, USA: Springer, 1995.

[17] R. Elliott, W. Hunter, and B. Jamieson, Financial signal processing: A
self-calibrating model, Int. J. Theor. Appl. Finance, vol. 4, pp. 567584,
2001.

[18] R. Elliott and C. Wilson, The term structure of interest rates in a hidden
Markov setting, in Hidden Markov Models in Finance, R. Mamon and
R. Elliott, Eds. New York, NY, USA: Springer, 2007.

[19] C. Erlwein and R. Mamon, An online estimation scheme for Hull-White
model with HMM-driven parameters, Stat. Methods Appl., vol. 18, no. 1,
pp. 87107, 2009.

[20] C. Erlwein, F. Benth, and R. Mamon, HMM ltering and parameter esti-
mation of an electricity spot price model, Energy Econ., vol. 32, no. 5,
pp. 10341043, 2010.

[21] C. Erlwein, R. Mamon, and M. Davison, An examination of HMM-
based investment strategies for asset allocation, Appl. Stochastic Models
Bus. Ind., vol. 27, pp. 204221, 2011.

[22] G. Gorton, Misunderstanding Financial Crises: Why We Dont See Them

Coming. London, U.K.: Oxford Univ. Press, 2012.

[23] R. Goyenko.

(2013). Treasury Liquidity and Funding Liquidity:
[Online]. Available:

Returns

Evidence
http://dx.doi.org/10.2139/ssrn.2023187

From Mutual

Fund

[24] J. Hamilton, Time Series Analysis. Princeton, NJ, USA: Princeton Univ.

Press, 1994.

[25] M. Hardy, A regime-switching model of long-term stock returns, North

Amer. Actuarial J., vol. 6, no. 1, pp. 171173, 2002.

[26] N. Hughes. (2005). A Trade War With China. Foreign Affairs [Online].
Available: http://www.foreignaffairs.com/articles/60825/neil-c-hughes/a-
trade-war-with-china

[27] J. James and N. Webber, Interest Rate Modelling. Hoboken, NJ, USA:

Wiley, 2000.

[28] P. Krugman, The coincidence of a liberalMission not accom-
plished, not yet anyway, New York Times, Mar. 12, 2008 [Online].
Available:
http://krugman.blogs.nytimes.com/2008/03/12/mission-not-
accomplished-not-yet-anyway/

[29] F. Longin and B. Solnik, Extreme correlation of international equity

markets, J. Finance, vol. 56, no. 2, pp. 649676, 2001.

[30] L. Mancini, A. Ranaldo, and J. Wrampelmeyer. (2012). The Foreign
Exchange Market: Not as Liquid as You May Think [Online]. Available:
http://www.voxeu.org/article/foreign-exchange-market-not-liquid-you-
may-think

[31] G. Schwarz, Estimating the dimension of a model, Ann. Statist., vol. 6,

no. 2, pp. 461464, 1978.

[32] S. Shreve, Stochastic Calculus for Finance II: Continuous Time Models.

New York, NY, USA: Springer, 2004.

Trading and Investing. New York, NY, USA: Bloomberg Press, 2009.

[34] J. W. van der End and M. Tabbae, When liquidity risk becomes a sys-
temic issue: Empirical evidence of bank behaviour, J. Financial Stab.,
vol. 8, pp. 107120, 2012.

[35] D. Vayanos and J. Wand, Market

liquidityTheory and empiri-
cal evidence, in Handbook of the Economics of Finance: Financial
Markets and Asset Pricing, G. Constantinides, R. Stulz, and M. Harris.
Amsterdam, The Netherlands: Elsevier, 2013, pp. 12891361.

[36] X. Xi and R. Mamon, Parameter estimation of an asset price model
driven by a weak hidden Markov chain, Econ. Modell., vol. 28, pp. 36
46, 2011.

[37] X. Xi and R. Mamon, Yield curve modelling using a multivariate higher-
order HMM, in State-Space Models and Applications in Economics and
Finance, Y. Zeng and S. Wu, Eds. New York, NY, USA: Springer, 2013,
pp. 185202.

[38] N. Zhou and R. Mamon, An accessible implementation of interest rate
models with Markov-switching, Expert Syst. Appl., vol. 39, pp. 4679
4689, 2012.

Anton Tenyakov received the B.Sc. degree in applied
mathematics (rst class with distinction) and the
M.A. degree in statistics (with concentration in
Financial Mathematics) both from York University,
Toronto, ON, Canada, and the Ph.D. degree from the
University of Western Ontario, London, ON, Canada.
He is a Manager, Financial Modeling in the Treasury
and Balance Sheet Management Department, TD
Bank Group, Toronto, ON, Canada. His research
interests include the applications of HMM ltering
methods in the parameter estimation of new and

extended regime-switching-based models for various nancial markets.

Rogemar Mamon (M16) is a Professor with the
Department of Statistical and Actuarial Sciences,
University of Western Ontario, London, ON, Canada.
His research interests include the applications of
stochastic processes to nancial and actuarial mod-
eling, hidden Markov models and their estimation
including ltering, smoothing and prediction, and
inverse problems in nance. He is a Co-Editor of the
IMA Journal of Management Mathematics (Oxford
University Press). He is an elected Fellow of the
U.K.s Institute of Mathematics and its Applications

and Chartered Scientist of the U.K.s Science Council.

Matt Davison received the degrees in engineer-
ing and maths. He was previously a Front Ofce
Desk Quant with Deutsche Bank Canada. He is
a Professor with the Department of Statistical
and Actuarial Sciences and the Department of
Applied Mathematics, University of Western Ontario,
London, ON, Canada. He has held the Canada
Research Chair in Quantitative Finance and is a
Fellow of
the Fields Institute for Research in
Mathematical Sciences, Toronto, ON, Canada. His
research interests include computational nance,
incomplete market theory, and energy nance. He continues to consult with
industry and serves as the Director and an Advisor to several Toronto-area
nancial companies.

